{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Mining Project<br> Data Science Degree | Outumn semester</h3> \n",
    "<i>With the orientation of professors Carina Albuquerque, Artur Varanda, Mohamed Ebowab and Ricardo Santos</i><br><br>\n",
    "\n",
    "<center><font color='#F16007'><font><h1>Decoding the Rythms of Emotion: </h1></font>\n",
    "    <font color='#FF914D'><font><h1>A Sentimental Journey through Music Genres</h1></font></center><br>\n",
    "\n",
    "   \n",
    "<br><div style=\"text-align: justify\"> Through the lens of Natural Language Preprocesseng (NLP) we want to complete this project with the goal of creating a classification model that can accurately predict the genre of a song based on its lyrics, investigating the relationship between a song's music lyrics and its genre and to perform a sentiment analysis, trying to uncover the emotional undertones present within each song.<br>In this Notebook we perform the Genre Identification, being the objective to create a model capable of identify the songs genre by their lyrics. </div>\n",
    "    \n",
    "<br>__Note:__ Our project code is divided into 3 Notebooks: 01_EDA, 02_Genre_Identification, 03_Sentiment_Analysis and a .py document named functions.\n",
    "    \n",
    "<br> \n",
    "    <br><br>Delivered by:\n",
    " \n",
    "    António Oliveira | 20211595\n",
    "    David Martins    | 20211628\n",
    "    Mariana Ferreira | 20211637\n",
    "    Mariana Takimura | 20211619\n",
    "    Rui Lourenço     | 20211639\n",
    "<br>\n",
    "   \n",
    "\n",
    "**********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Text-Mining-Project-Data-Science-Degree-|-Outumn-semester\" data-toc-modified-id=\"Text-Mining-Project-Data-Science-Degree-|-Outumn-semester-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Text Mining Project<br> Data Science Degree | Outumn semester</a></span></li></ul></li><li><span><a href=\"#Table-of-Contents\" data-toc-modified-id=\"Table-of-Contents-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>Table of Contents</font></font></a></span></li><li><span><a href=\"#1.-Importing-Libraries\" data-toc-modified-id=\"1.-Importing-Libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>1. Importing Libraries</font></font></a></span></li><li><span><a href=\"#2.-Loading-the-data\" data-toc-modified-id=\"2.-Loading-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>2. Loading the data</font></font></a></span></li><li><span><a href=\"#3.-Preprocessing\" data-toc-modified-id=\"3.-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>3. Preprocessing</font></font></a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1.-Stopwords\" data-toc-modified-id=\"3.1.-Stopwords-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>3.1. Stopwords</font></font></a></span></li></ul></li><li><span><a href=\"#4.-Vectorization\" data-toc-modified-id=\"4.-Vectorization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>4. Vectorization</font></font></a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Bag-of-Words-(BoW)\" data-toc-modified-id=\"4.1-Bag-of-Words-(BoW)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>4.1 Bag of Words (BoW)</font></font></a></span></li><li><span><a href=\"#4.2.-TF-IDF\" data-toc-modified-id=\"4.2.-TF-IDF-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>4.2. TF-IDF</font></font></a></span></li><li><span><a href=\"#4.3.-Document-embeddings-(Doc2Vec)\" data-toc-modified-id=\"4.3.-Document-embeddings-(Doc2Vec)-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>4.3. Document embeddings (Doc2Vec)</font></font></a></span></li></ul></li><li><span><a href=\"#5.-Model-Training\" data-toc-modified-id=\"5.-Model-Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>5. Model Training</font></font></a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1.-Logistic-Regression\" data-toc-modified-id=\"5.1.-Logistic-Regression-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>5.1. Logistic Regression</font></font></a></span></li><li><span><a href=\"#5.2.-MultinomialNB\" data-toc-modified-id=\"5.2.-MultinomialNB-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>5.2. MultinomialNB</font></font></a></span></li><li><span><a href=\"#5.3.-Neural-Networks\" data-toc-modified-id=\"5.3.-Neural-Networks-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>5.3. Neural Networks</font></font></a></span></li></ul></li><li><span><a href=\"#6.-Genre-Identification\" data-toc-modified-id=\"6.-Genre-Identification-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>6. Genre Identification</font></font></a></span></li><li><span><a href=\"#7.-Export-Results\" data-toc-modified-id=\"7.-Export-Results-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span><font color=\"#FE7845\"><font>7. Export Results</font></font></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "<font color='#FE7845'><font><h2>Table of Contents</h2></font>\n",
    "1. [Importing Libraries](#1)\n",
    "1. [Loading the data](#2)\n",
    "1. [Preprocessing](#3) <br>\n",
    "    3.1. [Stopwords](#4) <br>\n",
    "1. [Vectorization](#5) <br>\n",
    "    4.1. [Bag of Words Bow (BoW)](#6)<br>\n",
    "    4.2. [TF-IDF](#7) <br>\n",
    "    4.3. [Document embeddings (Doc2Vec)](#8) <br>\n",
    "1. [Model Training](#9) <br>\n",
    "    5.1. [Logistic Regression](#10) <br>\n",
    "    5.2. [MultinomialNB](#11) <br>\n",
    "    5.3. [Neural Network](#12)\n",
    "1. [Genre Identification](#13) <br>\n",
    "1. [Export Results](#14) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class='anchor' id='1'></a>\n",
    "<font color='#FE7845'><font><h2>1. Importing Libraries</h2></font>\n",
    "[Top &#129033;](#0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:52:40.386846Z",
     "start_time": "2023-12-21T12:52:30.987412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The functions.py file was imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "import regex as re\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "import unicodedata\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import ast\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\n",
    "\n",
    "# \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Functions File\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class='anchor' id='2'></a>\n",
    "<font color='#FE7845'><font><h2>2. Loading the data</h2></font>\n",
    "[Top &#129033;](#0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:53:11.956647Z",
     "start_time": "2023-12-21T12:53:02.577501Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:53:11.988567Z",
     "start_time": "2023-12-21T12:53:11.959226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_length</th>\n",
       "      <th>lyrics_extracted</th>\n",
       "      <th>clean_for_visualisations</th>\n",
       "      <th>lemmatized_lyrics</th>\n",
       "      <th>stemmed_lyrics</th>\n",
       "      <th>clean_lyrics</th>\n",
       "      <th>clean_lyrics_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walk Away</td>\n",
       "      <td>rock</td>\n",
       "      <td>Tony Molina</td>\n",
       "      <td>2013</td>\n",
       "      <td>699</td>\n",
       "      <td>{}</td>\n",
       "      <td>When you said you loved me\\nDid you mean it th...</td>\n",
       "      <td>193</td>\n",
       "      <td>[]</td>\n",
       "      <td>When said loved Did mean Did change mind one d...</td>\n",
       "      <td>['say', 'love', 'mean', 'change', 'mind', 'one...</td>\n",
       "      <td>['said', 'love', 'mean', 'chang', 'mind', 'one...</td>\n",
       "      <td>['said', 'loved', 'mean', 'change', 'mind', 'o...</td>\n",
       "      <td>When said loved Did mean Did change mind one d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gotta Make It Kid Naruto Rap</td>\n",
       "      <td>rap</td>\n",
       "      <td>Reece Lett</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>{Sl!ck}</td>\n",
       "      <td>Kid Naruto Rap\\n[Hook]\\nEverybody wants you to...</td>\n",
       "      <td>2143</td>\n",
       "      <td>[]</td>\n",
       "      <td>Kid Naruto Rap Everybody wants hurt Everybody ...</td>\n",
       "      <td>['kid', 'naruto', 'rap', 'everybody', 'want', ...</td>\n",
       "      <td>['kid', 'naruto', 'rap', 'everybodi', 'want', ...</td>\n",
       "      <td>['kid', 'naruto', 'rap', 'everybody', 'wants',...</td>\n",
       "      <td>Kid Naruto Rap Everybody wants hurt Everybody ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>​this is what i asked for</td>\n",
       "      <td>pop</td>\n",
       "      <td>Elliot (DNK)</td>\n",
       "      <td>2019</td>\n",
       "      <td>389</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Verse 1]\\nPeople tell me I've changed\\nI find...</td>\n",
       "      <td>644</td>\n",
       "      <td>[]</td>\n",
       "      <td>People tell changed find hard explain Maybe fe...</td>\n",
       "      <td>['people', 'tell', 'change', 'find', 'hard', '...</td>\n",
       "      <td>['peopl', 'tell', 'chang', 'find', 'hard', 'ex...</td>\n",
       "      <td>['people', 'tell', 'changed', 'find', 'hard', ...</td>\n",
       "      <td>People tell changed find hard explain Maybe fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stealing Hearts</td>\n",
       "      <td>pop</td>\n",
       "      <td>Katie Armiger</td>\n",
       "      <td>2013</td>\n",
       "      <td>126</td>\n",
       "      <td>{}</td>\n",
       "      <td>You've been warned about me\\nDon't try to get ...</td>\n",
       "      <td>1387</td>\n",
       "      <td>[]</td>\n",
       "      <td>You warned Don try get close know want chain l...</td>\n",
       "      <td>['warn', 'try', 'get', 'close', 'know', 'want'...</td>\n",
       "      <td>['warn', 'tri', 'get', 'close', 'know', 'want'...</td>\n",
       "      <td>['warned', 'try', 'get', 'close', 'know', 'wan...</td>\n",
       "      <td>You warned Don try get close know want chain l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Right There</td>\n",
       "      <td>rap</td>\n",
       "      <td>HoustonBlake</td>\n",
       "      <td>2015</td>\n",
       "      <td>61</td>\n",
       "      <td>{}</td>\n",
       "      <td>Oh yeah\\nOh yeah yeah yeah\\nOh yeah\\nRight the...</td>\n",
       "      <td>1194</td>\n",
       "      <td>[]</td>\n",
       "      <td>Oh yeah Oh yeah yeah yeah Oh yeah Right awh An...</td>\n",
       "      <td>['oh', 'yeah', 'oh', 'yeah', 'yeah', 'yeah', '...</td>\n",
       "      <td>['oh', 'yeah', 'oh', 'yeah', 'yeah', 'yeah', '...</td>\n",
       "      <td>['oh', 'yeah', 'oh', 'yeah', 'yeah', 'yeah', '...</td>\n",
       "      <td>Oh yeah Oh yeah yeah yeah Oh yeah Right awh An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133900</th>\n",
       "      <td>Manhattan</td>\n",
       "      <td>pop</td>\n",
       "      <td>Tijuana Sweetheart</td>\n",
       "      <td>2007</td>\n",
       "      <td>48</td>\n",
       "      <td>{}</td>\n",
       "      <td>If I knew when I was young that I'd be older\\n...</td>\n",
       "      <td>1460</td>\n",
       "      <td>[]</td>\n",
       "      <td>If knew young older There many things would to...</td>\n",
       "      <td>['know', 'young', 'old', 'many', 'thing', 'wou...</td>\n",
       "      <td>['knew', 'young', 'older', 'mani', 'thing', 'w...</td>\n",
       "      <td>['knew', 'young', 'older', 'many', 'things', '...</td>\n",
       "      <td>If knew young older There many things would to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133901</th>\n",
       "      <td>Belly Shit</td>\n",
       "      <td>rap</td>\n",
       "      <td>Lil Gotit</td>\n",
       "      <td>2019</td>\n",
       "      <td>3593</td>\n",
       "      <td>{\"Lil Troup\"}</td>\n",
       "      <td>[Intro: Lil Gotit]\\nCash\\nWah-wah-wah\\nWah-wah...</td>\n",
       "      <td>4247</td>\n",
       "      <td>['Drip god', 'I wear that\\u2005shit', 'The cut...</td>\n",
       "      <td>Cash Wah wah wah Wah wah wah Everything chrome...</td>\n",
       "      <td>['cash', 'wah', 'wah', 'wah', 'wah', 'wah', 'w...</td>\n",
       "      <td>['cash', 'wah', 'wah', 'wah', 'wah', 'wah', 'w...</td>\n",
       "      <td>['cash', 'wah', 'wah', 'wah', 'wah', 'wah', 'w...</td>\n",
       "      <td>Cash Wah wah wah Wah wah wah Everything chrome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133902</th>\n",
       "      <td>The Four Loves</td>\n",
       "      <td>rock</td>\n",
       "      <td>Heath McNease</td>\n",
       "      <td>2014</td>\n",
       "      <td>301</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Verse 1]\\nI was born inside a home\\nThe young...</td>\n",
       "      <td>1091</td>\n",
       "      <td>[]</td>\n",
       "      <td>born inside home The youngest three Boys fough...</td>\n",
       "      <td>['bear', 'inside', 'home', 'young', 'three', '...</td>\n",
       "      <td>['born', 'insid', 'home', 'youngest', 'three',...</td>\n",
       "      <td>['born', 'inside', 'home', 'youngest', 'three'...</td>\n",
       "      <td>born inside home The youngest three Boys fough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133903</th>\n",
       "      <td>Ball And Chain</td>\n",
       "      <td>pop</td>\n",
       "      <td>Transmatic</td>\n",
       "      <td>2001</td>\n",
       "      <td>46</td>\n",
       "      <td>{}</td>\n",
       "      <td>Rollin' by the pool the falling stars are not ...</td>\n",
       "      <td>1283</td>\n",
       "      <td>['CHORUS:', 'CHORUS:', 'are you the one for me...</td>\n",
       "      <td>Rollin pool falling stars condescending It twi...</td>\n",
       "      <td>['rollin', 'pool', 'fall', 'star', 'condescend...</td>\n",
       "      <td>['rollin', 'pool', 'fall', 'star', 'condescend...</td>\n",
       "      <td>['rollin', 'pool', 'falling', 'stars', 'condes...</td>\n",
       "      <td>Rollin pool falling stars condescending It twi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133904</th>\n",
       "      <td>It Aint Epic Enough</td>\n",
       "      <td>rap</td>\n",
       "      <td>TheGodlyRiskTakers</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Verse 1: GodlyRiskTaker A]\\nYeah Epic Games, ...</td>\n",
       "      <td>2399</td>\n",
       "      <td>[]</td>\n",
       "      <td>Yeah Epic Games playin games Now set record st...</td>\n",
       "      <td>['yeah', 'epic', 'game', 'playin', 'game', 'se...</td>\n",
       "      <td>['yeah', 'epic', 'game', 'playin', 'game', 'se...</td>\n",
       "      <td>['yeah', 'epic', 'games', 'playin', 'games', '...</td>\n",
       "      <td>Yeah Epic Games playin games Now set record st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133905 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title   tag              artist  year  views  \\\n",
       "0                          Walk Away  rock         Tony Molina  2013    699   \n",
       "1       Gotta Make It Kid Naruto Rap   rap          Reece Lett  2021      4   \n",
       "2          ​this is what i asked for   pop        Elliot (DNK)  2019    389   \n",
       "3                    Stealing Hearts   pop       Katie Armiger  2013    126   \n",
       "4                        Right There   rap        HoustonBlake  2015     61   \n",
       "...                              ...   ...                 ...   ...    ...   \n",
       "133900                     Manhattan   pop  Tijuana Sweetheart  2007     48   \n",
       "133901                    Belly Shit   rap           Lil Gotit  2019   3593   \n",
       "133902                The Four Loves  rock       Heath McNease  2014    301   \n",
       "133903                Ball And Chain   pop          Transmatic  2001     46   \n",
       "133904           It Aint Epic Enough   rap  TheGodlyRiskTakers  2020      6   \n",
       "\n",
       "             features                                             lyrics  \\\n",
       "0                  {}  When you said you loved me\\nDid you mean it th...   \n",
       "1             {Sl!ck}  Kid Naruto Rap\\n[Hook]\\nEverybody wants you to...   \n",
       "2                  {}  [Verse 1]\\nPeople tell me I've changed\\nI find...   \n",
       "3                  {}  You've been warned about me\\nDon't try to get ...   \n",
       "4                  {}  Oh yeah\\nOh yeah yeah yeah\\nOh yeah\\nRight the...   \n",
       "...               ...                                                ...   \n",
       "133900             {}  If I knew when I was young that I'd be older\\n...   \n",
       "133901  {\"Lil Troup\"}  [Intro: Lil Gotit]\\nCash\\nWah-wah-wah\\nWah-wah...   \n",
       "133902             {}  [Verse 1]\\nI was born inside a home\\nThe young...   \n",
       "133903             {}  Rollin' by the pool the falling stars are not ...   \n",
       "133904             {}  [Verse 1: GodlyRiskTaker A]\\nYeah Epic Games, ...   \n",
       "\n",
       "        lyrics_length                                   lyrics_extracted  \\\n",
       "0                 193                                                 []   \n",
       "1                2143                                                 []   \n",
       "2                 644                                                 []   \n",
       "3                1387                                                 []   \n",
       "4                1194                                                 []   \n",
       "...               ...                                                ...   \n",
       "133900           1460                                                 []   \n",
       "133901           4247  ['Drip god', 'I wear that\\u2005shit', 'The cut...   \n",
       "133902           1091                                                 []   \n",
       "133903           1283  ['CHORUS:', 'CHORUS:', 'are you the one for me...   \n",
       "133904           2399                                                 []   \n",
       "\n",
       "                                 clean_for_visualisations  \\\n",
       "0       When said loved Did mean Did change mind one d...   \n",
       "1       Kid Naruto Rap Everybody wants hurt Everybody ...   \n",
       "2       People tell changed find hard explain Maybe fe...   \n",
       "3       You warned Don try get close know want chain l...   \n",
       "4       Oh yeah Oh yeah yeah yeah Oh yeah Right awh An...   \n",
       "...                                                   ...   \n",
       "133900  If knew young older There many things would to...   \n",
       "133901  Cash Wah wah wah Wah wah wah Everything chrome...   \n",
       "133902  born inside home The youngest three Boys fough...   \n",
       "133903  Rollin pool falling stars condescending It twi...   \n",
       "133904  Yeah Epic Games playin games Now set record st...   \n",
       "\n",
       "                                        lemmatized_lyrics  \\\n",
       "0       ['say', 'love', 'mean', 'change', 'mind', 'one...   \n",
       "1       ['kid', 'naruto', 'rap', 'everybody', 'want', ...   \n",
       "2       ['people', 'tell', 'change', 'find', 'hard', '...   \n",
       "3       ['warn', 'try', 'get', 'close', 'know', 'want'...   \n",
       "4       ['oh', 'yeah', 'oh', 'yeah', 'yeah', 'yeah', '...   \n",
       "...                                                   ...   \n",
       "133900  ['know', 'young', 'old', 'many', 'thing', 'wou...   \n",
       "133901  ['cash', 'wah', 'wah', 'wah', 'wah', 'wah', 'w...   \n",
       "133902  ['bear', 'inside', 'home', 'young', 'three', '...   \n",
       "133903  ['rollin', 'pool', 'fall', 'star', 'condescend...   \n",
       "133904  ['yeah', 'epic', 'game', 'playin', 'game', 'se...   \n",
       "\n",
       "                                           stemmed_lyrics  \\\n",
       "0       ['said', 'love', 'mean', 'chang', 'mind', 'one...   \n",
       "1       ['kid', 'naruto', 'rap', 'everybodi', 'want', ...   \n",
       "2       ['peopl', 'tell', 'chang', 'find', 'hard', 'ex...   \n",
       "3       ['warn', 'tri', 'get', 'close', 'know', 'want'...   \n",
       "4       ['oh', 'yeah', 'oh', 'yeah', 'yeah', 'yeah', '...   \n",
       "...                                                   ...   \n",
       "133900  ['knew', 'young', 'older', 'mani', 'thing', 'w...   \n",
       "133901  ['cash', 'wah', 'wah', 'wah', 'wah', 'wah', 'w...   \n",
       "133902  ['born', 'insid', 'home', 'youngest', 'three',...   \n",
       "133903  ['rollin', 'pool', 'fall', 'star', 'condescend...   \n",
       "133904  ['yeah', 'epic', 'game', 'playin', 'game', 'se...   \n",
       "\n",
       "                                             clean_lyrics  \\\n",
       "0       ['said', 'loved', 'mean', 'change', 'mind', 'o...   \n",
       "1       ['kid', 'naruto', 'rap', 'everybody', 'wants',...   \n",
       "2       ['people', 'tell', 'changed', 'find', 'hard', ...   \n",
       "3       ['warned', 'try', 'get', 'close', 'know', 'wan...   \n",
       "4       ['oh', 'yeah', 'oh', 'yeah', 'yeah', 'yeah', '...   \n",
       "...                                                   ...   \n",
       "133900  ['knew', 'young', 'older', 'many', 'things', '...   \n",
       "133901  ['cash', 'wah', 'wah', 'wah', 'wah', 'wah', 'w...   \n",
       "133902  ['born', 'inside', 'home', 'youngest', 'three'...   \n",
       "133903  ['rollin', 'pool', 'falling', 'stars', 'condes...   \n",
       "133904  ['yeah', 'epic', 'games', 'playin', 'games', '...   \n",
       "\n",
       "                                   clean_lyrics_sentiment  \n",
       "0       When said loved Did mean Did change mind one d...  \n",
       "1       Kid Naruto Rap Everybody wants hurt Everybody ...  \n",
       "2       People tell changed find hard explain Maybe fe...  \n",
       "3       You warned Don try get close know want chain l...  \n",
       "4       Oh yeah Oh yeah yeah yeah Oh yeah Right awh An...  \n",
       "...                                                   ...  \n",
       "133900  If knew young older There many things would to...  \n",
       "133901  Cash Wah wah wah Wah wah wah Everything chrome...  \n",
       "133902  born inside home The youngest three Boys fough...  \n",
       "133903  Rollin pool falling stars condescending It twi...  \n",
       "133904  Yeah Epic Games playin games Now set record st...  \n",
       "\n",
       "[133905 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class='anchor' id='3'></a>\n",
    "<font color='#FE7845'><font><h2>3. Preprocessing</h2></font>\n",
    "[Top &#129033;](#0) \n",
    "    \n",
    "<a class='anchor' id='4'></a>\n",
    "<font color='#FE7845'><font><h3>3.1. Stopwords</h3></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.921159Z",
     "start_time": "2023-12-21T12:15:11.921140Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we will be looking to expand the stopwords list. Firstly we create a list that will store the top words in each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.923160Z",
     "start_time": "2023-12-21T12:15:11.923134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rock, k = 250, column = clean_lyrics\n",
      "['know', 'like', 'oh', 'love', 'time', 'na', 'got', 'let', 'life', 'away', 'want', 'way', 'feel', 'come', 'yeah', 'say', 'make', 'cause', 'night', 'world', 'day', 'eyes', 'need', 'gon', 'heart', 'wan', 'right', 'think', 'tell', 'mind', 'baby', 'man', 'said', 'home', 'look', 'light', 'inside', 'head', 'end', 'long', 'left', 'good', 'things', 'die', 'lost', 'little', 'live', 'fall', 'hear', 'face', 'better', 'gone', 'run', 'try', 'leave', 'stay', 'dead', 'hold', 'la', 'god', 'place', 'going', 'hey', 'new', 'soul', 'blood', 'believe', 'pain', 'sun', 'stop', 'free', 'coming', 'tonight', 'turn', 'ooh', 'really', 'far', 'death', 'hope', 'girl', 'hard', 'hand', 'feeling', 'people', 'old', 'wrong', 'break', 'fight', 'black', 'sky', 'words', 'thing', 'days', 'maybe', 'change', 'cold', 'hands', 'hate', 'dark', 'ta', 'care', 'dream', 'thought', 'waiting', 'stand', 'fucking', 'walk', 'high', 'ah', 'hell', 'fuck', 'burn', 'fear', 'forever', 'lies', 'watch', 'wait', 'alive', 'bring', 'looking', 'dreams', 'comes', 'living', 'broken', 'help', 'sleep', 'open', 'lie', 'remember', 'truth', 'start', 'wish', 'bad', 'song', 'deep', 'ground', 'real', 'running', 'getting', 'best', 'friends', 'true', 'play', 'forget', 'sing', 'falling', 'lose', 'yes', 'save', 'seen', 'line', 'dance', 'close', 'goes', 'door', 'trying', 'alright', 'kill', 'rain', 'body', 'came', 'past', 'times', 'sound', 'hide', 'set', 'used', 'told', 'years', 'war', 'knew', 'makes', 'rock', 'friend', 'burning', 'today', 'talk', 'blue', 'fine', 'shit', 'earth', 'mean', 'saw', 'big', 'wanted', 'control', 'rise', 'took', 'late', 'lord', 'sure', 'morning', 'darkness', 'breath', 'ready', 'lonely', 'red', 'skin', 'da', 'anymore', 'road', 'ride', 'wake', 'feels', 'room', 'tears', 'apart', 'round', 'everybody', 'town', 'matter', 'stars', 'sea', 'follow', 'young', 'boy', 'water', 'feet', 'tried', 'born', 'guess', 'sick', 'sweet', 'smile', 'cut', 'bed', 'soon', 'air', 'reason', 'understand', 'money', 'till', 'easy', 'white', 'felt', 'wind', 'breathe', 'land', 'eye', 'heaven', 'power', 'knows', 'heard', 'chance', 'ya', 'kind', 'thinking', 'voice', 'happy', 'rest']\n",
      "Computing rap, k = 250, column = clean_lyrics\n",
      "['like', 'got', 'know', 'yeah', 'shit', 'na', 'bitch', 'fuck', 'nigga', 'niggas', 'cause', 'make', 'love', 'wan', 'time', 'man', 'want', 'let', 'gon', 'say', 'need', 'life', 'em', 'money', 'way', 'right', 'feel', 'ta', 'come', 'tell', 'oh', 'think', 'ya', 'really', 'baby', 'yo', 'hit', 'look', 'girl', 'said', 'real', 'tryna', 'uh', 'day', 'better', 'fucking', 'bout', 'new', 'im', 'mind', 'good', 'ass', 'boy', 'god', 'stay', 'big', 'bitches', 'run', 'stop', 'people', 'head', 'bad', 'getting', 'going', 'talk', 'hard', 'night', 'face', 'told', 'leave', 'play', 'try', 'world', 'pull', 'game', 'high', 'damn', 'gone', 'lil', 'verse', 'heart', 'away', 'live', 'fuckin', 'feeling', 'hate', 'watch', 'little', 'die', 'beat', 'hold', 'turn', 'came', 'yea', 'aye', 'black', 'rap', 'ayy', 'work', 'ride', 'best', 'cuz', 'talking', 'young', 'thing', 'long', 'eyes', 'left', 'dick', 'things', 'huh', 'hoes', 'thought', 'smoke', 'hear', 'looking', 'home', 'gang', 'pain', 'kill', 'body', 'pussy', 'wit', 'check', 'everybody', 'lot', 'lost', 'change', 'hope', 'drop', 'dead', 'yuh', 'mean', 'start', 'end', 'coming', 'hey', 'wait', 'till', 'bag', 'crazy', 'used', 'ooh', 'living', 'trying', 'care', 'pop', 'flow', 'light', 'break', 'friends', 'making', 'inside', 'mad', 'bring', 'cold', 'walk', 'city', 'old', 'took', 'roll', 'catch', 'soul', 'believe', 'straight', 'nah', 'hot', 'imma', 'seen', 'free', 'fall', 'hook', 'different', 'place', 'days', 'gettin', 'til', 'broke', 'block', 'round', 'music', 'fly', 'wrong', 'cash', 'hell', 'white', 'phone', 'running', 'blow', 'okay', 'guess', 'chorus', 'heard', 'dont', 'trust', 'low', 'ready', 'swear', 'ask', 'times', 'ay', 'rock', 'da', 'woah', 'fucked', 'cut', 'maybe', 'listen', 'line', 'house', 'remember', 'hand', 'fake', 'eat', 'pay', 'shoot', 'ah', 'throw', 'fast', 'hands', 'talkin', 'shot', 'went', 'thinking', 'streets', 'sleep', 'shawty', 'bro', 'dope', 'fight', 'song', 'couple', 'wish', 'hoe', 'gave', 'deep', 'goin', 'girls', 'knew', 'lose', 'lie', 'sick', 'hop', 'past', 'kid', 'king', 'dream', 'ha', 'help', 'red']\n",
      "Computing pop, k = 250, column = clean_lyrics\n",
      "['love', 'know', 'oh', 'like', 'na', 'got', 'time', 'yeah', 'let', 'want', 'come', 'baby', 'say', 'way', 'feel', 'make', 'cause', 'wan', 'life', 'need', 'away', 'gon', 'heart', 'right', 'night', 'tell', 'day', 'think', 'world', 'eyes', 'good', 'mind', 'girl', 'man', 'said', 'ooh', 'look', 'little', 'la', 'home', 'long', 'things', 'better', 'hold', 'hey', 'stay', 'light', 'really', 'leave', 'try', 'gone', 'tonight', 'head', 'hear', 'stop', 'new', 'run', 'fall', 'inside', 'believe', 'going', 'feeling', 'ta', 'live', 'left', 'god', 'turn', 'face', 'place', 'ah', 'lost', 'thing', 'end', 'people', 'hard', 'sun', 'free', 'maybe', 'bad', 'ya', 'wrong', 'thought', 'high', 'change', 'dream', 'break', 'soul', 'hand', 'forever', 'old', 'dance', 'die', 'far', 'hope', 'play', 'care', 'true', 'real', 'boy', 'days', 'sky', 'pain', 'walk', 'body', 'chorus', 'talk', 'looking', 'waiting', 'words', 'coming', 'hands', 'song', 'wait', 'wish', 'sing', 'remember', 'start', 'close', 'dreams', 'best', 'da', 'money', 'stand', 'cold', 'yes', 'told', 'friends', 'fight', 'comes', 'help', 'falling', 'living', 'forget', 'fuck', 'bring', 'alright', 'knew', 'open', 'running', 'lord', 'sweet', 'everybody', 'rain', 'lie', 'getting', 'dark', 'trying', 'big', 'blue', 'watch', 'lose', 'shit', 'came', 'fly', 'uh', 'times', 'touch', 'kiss', 'round', 'till', 'friend', 'feels', 'deep', 'alive', 'used', 'sleep', 'fine', 'smile', 'door', 'crazy', 'makes', 'today', 'hate', 'dead', 'miss', 'work', 'black', 'morning', 'mean', 'goes', 'sound', 'seen', 'lonely', 'ride', 'broken', 'sure', 'guess', 'blood', 'truth', 'thinking', 'took', 'stars', 'em', 'happy', 'ground', 'matter', 'hit', 'lies', 'ready', 'tears', 'line', 'saw', 'late', 'fear', 'hide', 'hell', 'beautiful', 'town', 'lights', 'rock', 'understand', 'young', 'wanted', 'years', 'save', 'somebody', 'knows', 'easy', 'heaven', 'goodbye', 'hurt', 'set', 'road', 'wake', 'bout', 'til', 'control', 'party', 'water', 'arms', 'past', 'listen', 'room', 'kind', 'shine', 'chance', 'beat', 'tried', 'ask', 'music', 'anymore', 'pretty', 'gave', 'game', 'slow', 'follow', 'wonder', 'heard', 'bed', 'feet']\n",
      "Computing country, k = 250, column = clean_lyrics\n",
      "['love', 'know', 'like', 'got', 'na', 'oh', 'time', 'way', 'yeah', 'let', 'gon', 'cause', 'baby', 'little', 'heart', 'say', 'come', 'night', 'good', 'man', 'away', 'want', 'home', 'make', 'old', 'right', 'day', 'long', 'said', 'life', 'tell', 'think', 'need', 'nerves', 'girl', 'wan', 'song', 'feel', 'world', 'gone', 'look', 'mind', 'eyes', 'left', 'town', 'things', 'hold', 'better', 'thing', 'road', 'big', 'leave', 'hard', 'tonight', 'lord', 'light', 'run', 'hear', 'new', 'ooh', 'stay', 'god', 'blue', 'country', 'boy', 'hand', 'believe', 'fall', 'live', 'wrong', 'thought', 'place', 'head', 'going', 'turn', 'sweet', 'days', 'sun', 'ta', 'lost', 'hey', 'told', 'wish', 'cold', 'really', 'people', 'high', 'end', 'rain', 'try', 'change', 'crazy', 'sing', 'walk', 'em', 'maybe', 'took', 'soul', 'hell', 'free', 'came', 'true', 'best', 'bad', 'used', 'drink', 'hands', 'woman', 'face', 'line', 'comes', 'sure', 'goes', 'red', 'wind', 'hope', 'door', 'kiss', 'morning', 'til', 'knew', 'coming', 'far', 'break', 'friends', 'lonely', 'times', 'dream', 'guess', 'years', 'yes', 'ride', 'stop', 'seen', 'pretty', 'ground', 'start', 'daddy', 'saw', 'close', 'play', 'round', 'help', 'die', 'sky', 'smile', 'miss', 'friend', 'knows', 'remember', 'water', 'dark', 'everybody', 'looking', 'makes', 'bout', 'lay', 'heaven', 'real', 'went', 'honey', 'mama', 'forever', 'goodbye', 'deep', 'river', 'today', 'feeling', 'black', 'roll', 'till', 'words', 'dreams', 'lights', 'fly', 'arms', 'white', 'inside', 'tears', 'bring', 'strong', 'wonder', 'happy', 'dance', 'boys', 'somebody', 'work', 'care', 'money', 'pain', 'living', 'wait', 'fine', 'train', 'stars', 'heard', 'moon', 'loved', 'talk', 'damn', 'ya', 'la', 'lot', 'city', 'jesus', 'ah', 'getting', 'stand', 'young', 'matter', 'anymore', 'fool', 'late', 'truck', 'born', 'easy', 'forget', 'kind', 'soon', 'mean', 'lose', 'waiting', 'fight', 'open', 'beer', 'running', 'broken', 'hurt', 'called', 'feet', 'gets', 'house', 'rock', 'whiskey', 'land', 'christmas', 'livin', 'sleep', 'hair', 'drive', 'watch', 'reason', 'mountain', 'chorus', 'nothin', 'wild', 'summer', 'gave', 'slow', 'son']\n",
      "Computing rb, k = 250, column = clean_lyrics\n",
      "['know', 'love', 'yeah', 'got', 'like', 'oh', 'baby', 'na', 'let', 'wan', 'want', 'girl', 'time', 'need', 'say', 'make', 'cause', 'feel', 'come', 'way', 'tell', 'gon', 'right', 'ooh', 'life', 'good', 'think', 'night', 'mind', 'heart', 'said', 'ta', 'day', 'away', 'man', 'really', 'hey', 'better', 'ya', 'look', 'shit', 'stay', 'hold', 'leave', 'things', 'fuck', 'real', 'la', 'feeling', 'world', 'eyes', 'long', 'little', 'ah', 'going', 'body', 'bad', 'bout', 'thing', 'tonight', 'gone', 'try', 'hard', 'tryna', 'stop', 'new', 'home', 'uh', 'boy', 'god', 'play', 'talk', 'thought', 'babe', 'crazy', 'hear', 'money', 'high', 'fall', 'nigga', 'believe', 'wrong', 'hit', 'care', 'face', 'told', 'wait', 'chorus', 'head', 'ride', 'inside', 'verse', 'live', 'em', 'best', 'yes', 'run', 'maybe', 'yea', 'change', 'looking', 'true', 'turn', 'lost', 'pain', 'people', 'break', 'left', 'miss', 'soul', 'bitch', 'knew', 'hope', 'forever', 'place', 'sweet', 'thinking', 'free', 'alright', 'close', 'end', 'help', 'light', 'friends', 'da', 'round', 'used', 'dance', 'mean', 'wish', 'damn', 'getting', 'ready', 'lie', 'loving', 'game', 'days', 'feels', 'times', 'woah', 'work', 'fine', 'lose', 'slow', 'im', 'phone', 'die', 'niggas', 'waiting', 'bring', 'feelings', 'low', 'touch', 'trying', 'remember', 'hands', 'lonely', 'coming', 'somebody', 'hate', 'yo', 'walk', 'far', 'cuz', 'lord', 'hand', 'deep', 'dream', 'girls', 'trust', 'came', 'cool', 'hurt', 'guess', 'fucking', 'start', 'sure', 'cold', 'sun', 'till', 'making', 'late', 'forget', 'pull', 'shawty', 'song', 'til', 'kiss', 'fly', 'hook', 'sorry', 'everybody', 'music', 'smile', 'watch', 'whoa', 'okay', 'aye', 'promise', 'party', 'listen', 'understand', 'lay', 'old', 'gave', 'matter', 'talking', 'goes', 'big', 'running', 'roll', 'wanted', 'happy', 'saying', 'morning', 'sleep', 'sing', 'living', 'seen', 'beat', 'falling', 'control', 'different', 'lot', 'words', 'friend', 'makes', 'mama', 'huh', 'ass', 'loved', 'ask', 'truth', 'calling', 'swear', 'tired', 'blue', 'sky', 'took', 'mi', 'save', 'stand', 'dreams', 'imma', 'young', 'bed', 'doo', 'fight', 'reason', 'knows']\n",
      "Computing misc, k = 250, column = clean_lyrics\n",
      "['like', 'know', 'said', 'time', 'man', 'people', 'come', 'love', 'got', 'way', 'say', 'good', 'let', 'life', 'make', 'day', 'right', 'shall', 'think', 'want', 'oh', 'world', 'little', 'new', 'god', 'long', 'away', 'great', 'old', 'going', 'things', 'men', 'look', 'came', 'tell', 'night', 'na', 'need', 'yeah', 'thought', 'house', 'eyes', 'work', 'place', 'years', 'heart', 'really', 'thing', 'head', 'went', 'hand', 'face', 'thou', 'mind', 'feel', 'father', 'home', 'left', 'light', 'white', 'lord', 'thy', 'better', 'black', 'saw', 'best', 'told', 'cause', 'mother', 'king', 'end', 'took', 'high', 'thee', 'gon', 'far', 'jerry', 'help', 'room', 'hands', 'yes', 'body', 'hear', 'water', 'woman', 'use', 'days', 'live', 'year', 'death', 'heard', 'young', 'words', 'girl', 'power', 'believe', 'son', 'door', 'earth', 'set', 'seen', 'mean', 'state', 'sir', 'called', 'money', 'children', 'mr', 'boy', 'used', 'dead', 'george', 'care', 'leave', 'big', 'looking', 'looked', 'hard', 'true', 'times', 'free', 'talk', 'knew', 'human', 'kind', 'gave', 'hope', 'land', 'blood', 'gone', 'reason', 'open', 'different', 'real', 'says', 'wan', 'country', 'lost', 'order', 'coming', 'sea', 'moment', 'matter', 'baby', 'soul', 'family', 'asked', 'city', 'play', 'sure', 'war', 'dream', 'hold', 'soon', 'friends', 'sun', 'try', 'voice', 'air', 'dark', 'music', 'half', 'bad', 'run', 'change', 'today', 'person', 'law', 'word', 'shit', 'school', 'stop', 'friend', 'remember', 'ye', 'till', 'case', 'turn', 'women', 'ah', 'song', 'child', 'felt', 'saying', 'comes', 'idea', 'lot', 'round', 'morning', 'understand', 'began', 'past', 'nature', 'stay', 'die', 'means', 'fact', 'given', 'bring', 'read', 'able', 'makes', 'th', 'trying', 'ask', 'sleep', 'states', 'brought', 'course', 'feet', 'wanted', 'wife', 'making', 'point', 'number', 'fall', 'unto', 'second', 'truth', 'hey', 'stand', 'speak', 'living', 'fear', 'turned', 'small', 'fuck', 'rest', 'certain', 'maybe', 'wrong', 'cold', 'ground', 'government', 'sound', 'bed', 'deep', 'inside', 'elaine', 'later', 'story', 'general', 'uh', 'getting', 'hair', 'wish', 'taken', 'red', 'brother', 'cut']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['know',\n",
       " 'like',\n",
       " 'oh',\n",
       " 'love',\n",
       " 'time',\n",
       " 'na',\n",
       " 'got',\n",
       " 'let',\n",
       " 'life',\n",
       " 'away',\n",
       " 'want',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'come',\n",
       " 'yeah',\n",
       " 'say',\n",
       " 'make',\n",
       " 'cause',\n",
       " 'night',\n",
       " 'world',\n",
       " 'day',\n",
       " 'eyes',\n",
       " 'need',\n",
       " 'gon',\n",
       " 'heart',\n",
       " 'wan',\n",
       " 'right',\n",
       " 'think',\n",
       " 'tell',\n",
       " 'mind',\n",
       " 'baby',\n",
       " 'man',\n",
       " 'said',\n",
       " 'home',\n",
       " 'look',\n",
       " 'light',\n",
       " 'inside',\n",
       " 'head',\n",
       " 'end',\n",
       " 'long',\n",
       " 'left',\n",
       " 'good',\n",
       " 'things',\n",
       " 'die',\n",
       " 'lost',\n",
       " 'little',\n",
       " 'live',\n",
       " 'fall',\n",
       " 'hear',\n",
       " 'face',\n",
       " 'better',\n",
       " 'gone',\n",
       " 'run',\n",
       " 'try',\n",
       " 'leave',\n",
       " 'stay',\n",
       " 'dead',\n",
       " 'hold',\n",
       " 'la',\n",
       " 'god',\n",
       " 'place',\n",
       " 'going',\n",
       " 'hey',\n",
       " 'new',\n",
       " 'soul',\n",
       " 'blood',\n",
       " 'believe',\n",
       " 'pain',\n",
       " 'sun',\n",
       " 'stop',\n",
       " 'free',\n",
       " 'coming',\n",
       " 'tonight',\n",
       " 'turn',\n",
       " 'ooh',\n",
       " 'really',\n",
       " 'far',\n",
       " 'death',\n",
       " 'hope',\n",
       " 'girl',\n",
       " 'hard',\n",
       " 'hand',\n",
       " 'feeling',\n",
       " 'people',\n",
       " 'old',\n",
       " 'wrong',\n",
       " 'break',\n",
       " 'fight',\n",
       " 'black',\n",
       " 'sky',\n",
       " 'words',\n",
       " 'thing',\n",
       " 'days',\n",
       " 'maybe',\n",
       " 'change',\n",
       " 'cold',\n",
       " 'hands',\n",
       " 'hate',\n",
       " 'dark',\n",
       " 'ta',\n",
       " 'care',\n",
       " 'dream',\n",
       " 'thought',\n",
       " 'waiting',\n",
       " 'stand',\n",
       " 'fucking',\n",
       " 'walk',\n",
       " 'high',\n",
       " 'ah',\n",
       " 'hell',\n",
       " 'fuck',\n",
       " 'burn',\n",
       " 'fear',\n",
       " 'forever',\n",
       " 'lies',\n",
       " 'watch',\n",
       " 'wait',\n",
       " 'alive',\n",
       " 'bring',\n",
       " 'looking',\n",
       " 'dreams',\n",
       " 'comes',\n",
       " 'living',\n",
       " 'broken',\n",
       " 'help',\n",
       " 'sleep',\n",
       " 'open',\n",
       " 'lie',\n",
       " 'remember',\n",
       " 'truth',\n",
       " 'start',\n",
       " 'wish',\n",
       " 'bad',\n",
       " 'song',\n",
       " 'deep',\n",
       " 'ground',\n",
       " 'real',\n",
       " 'running',\n",
       " 'getting',\n",
       " 'best',\n",
       " 'friends',\n",
       " 'true',\n",
       " 'play',\n",
       " 'forget',\n",
       " 'sing',\n",
       " 'falling',\n",
       " 'lose',\n",
       " 'yes',\n",
       " 'save',\n",
       " 'seen',\n",
       " 'line',\n",
       " 'dance',\n",
       " 'close',\n",
       " 'goes',\n",
       " 'door',\n",
       " 'trying',\n",
       " 'alright',\n",
       " 'kill',\n",
       " 'rain',\n",
       " 'body',\n",
       " 'came',\n",
       " 'past',\n",
       " 'times',\n",
       " 'sound',\n",
       " 'hide',\n",
       " 'set',\n",
       " 'used',\n",
       " 'told',\n",
       " 'years',\n",
       " 'war',\n",
       " 'knew',\n",
       " 'makes',\n",
       " 'rock',\n",
       " 'friend',\n",
       " 'burning',\n",
       " 'today',\n",
       " 'talk',\n",
       " 'blue',\n",
       " 'fine',\n",
       " 'shit',\n",
       " 'earth',\n",
       " 'mean',\n",
       " 'saw',\n",
       " 'big',\n",
       " 'wanted',\n",
       " 'control',\n",
       " 'rise',\n",
       " 'took',\n",
       " 'late',\n",
       " 'lord',\n",
       " 'sure',\n",
       " 'morning',\n",
       " 'darkness',\n",
       " 'breath',\n",
       " 'ready',\n",
       " 'lonely',\n",
       " 'red',\n",
       " 'skin',\n",
       " 'da',\n",
       " 'anymore',\n",
       " 'road',\n",
       " 'ride',\n",
       " 'wake',\n",
       " 'feels',\n",
       " 'room',\n",
       " 'tears',\n",
       " 'apart',\n",
       " 'round',\n",
       " 'everybody',\n",
       " 'town',\n",
       " 'matter',\n",
       " 'stars',\n",
       " 'sea',\n",
       " 'follow',\n",
       " 'young',\n",
       " 'boy',\n",
       " 'water',\n",
       " 'feet',\n",
       " 'tried',\n",
       " 'born',\n",
       " 'guess',\n",
       " 'sick',\n",
       " 'sweet',\n",
       " 'smile',\n",
       " 'cut',\n",
       " 'bed',\n",
       " 'soon',\n",
       " 'air',\n",
       " 'reason',\n",
       " 'understand',\n",
       " 'money',\n",
       " 'till',\n",
       " 'easy',\n",
       " 'white',\n",
       " 'felt',\n",
       " 'wind',\n",
       " 'breathe',\n",
       " 'land',\n",
       " 'eye',\n",
       " 'heaven',\n",
       " 'power',\n",
       " 'knows',\n",
       " 'heard',\n",
       " 'chance',\n",
       " 'ya',\n",
       " 'kind',\n",
       " 'thinking',\n",
       " 'voice',\n",
       " 'happy',\n",
       " 'rest',\n",
       " 'like',\n",
       " 'got',\n",
       " 'know',\n",
       " 'yeah',\n",
       " 'shit',\n",
       " 'na',\n",
       " 'bitch',\n",
       " 'fuck',\n",
       " 'nigga',\n",
       " 'niggas',\n",
       " 'cause',\n",
       " 'make',\n",
       " 'love',\n",
       " 'wan',\n",
       " 'time',\n",
       " 'man',\n",
       " 'want',\n",
       " 'let',\n",
       " 'gon',\n",
       " 'say',\n",
       " 'need',\n",
       " 'life',\n",
       " 'em',\n",
       " 'money',\n",
       " 'way',\n",
       " 'right',\n",
       " 'feel',\n",
       " 'ta',\n",
       " 'come',\n",
       " 'tell',\n",
       " 'oh',\n",
       " 'think',\n",
       " 'ya',\n",
       " 'really',\n",
       " 'baby',\n",
       " 'yo',\n",
       " 'hit',\n",
       " 'look',\n",
       " 'girl',\n",
       " 'said',\n",
       " 'real',\n",
       " 'tryna',\n",
       " 'uh',\n",
       " 'day',\n",
       " 'better',\n",
       " 'fucking',\n",
       " 'bout',\n",
       " 'new',\n",
       " 'im',\n",
       " 'mind',\n",
       " 'good',\n",
       " 'ass',\n",
       " 'boy',\n",
       " 'god',\n",
       " 'stay',\n",
       " 'big',\n",
       " 'bitches',\n",
       " 'run',\n",
       " 'stop',\n",
       " 'people',\n",
       " 'head',\n",
       " 'bad',\n",
       " 'getting',\n",
       " 'going',\n",
       " 'talk',\n",
       " 'hard',\n",
       " 'night',\n",
       " 'face',\n",
       " 'told',\n",
       " 'leave',\n",
       " 'play',\n",
       " 'try',\n",
       " 'world',\n",
       " 'pull',\n",
       " 'game',\n",
       " 'high',\n",
       " 'damn',\n",
       " 'gone',\n",
       " 'lil',\n",
       " 'verse',\n",
       " 'heart',\n",
       " 'away',\n",
       " 'live',\n",
       " 'fuckin',\n",
       " 'feeling',\n",
       " 'hate',\n",
       " 'watch',\n",
       " 'little',\n",
       " 'die',\n",
       " 'beat',\n",
       " 'hold',\n",
       " 'turn',\n",
       " 'came',\n",
       " 'yea',\n",
       " 'aye',\n",
       " 'black',\n",
       " 'rap',\n",
       " 'ayy',\n",
       " 'work',\n",
       " 'ride',\n",
       " 'best',\n",
       " 'cuz',\n",
       " 'talking',\n",
       " 'young',\n",
       " 'thing',\n",
       " 'long',\n",
       " 'eyes',\n",
       " 'left',\n",
       " 'dick',\n",
       " 'things',\n",
       " 'huh',\n",
       " 'hoes',\n",
       " 'thought',\n",
       " 'smoke',\n",
       " 'hear',\n",
       " 'looking',\n",
       " 'home',\n",
       " 'gang',\n",
       " 'pain',\n",
       " 'kill',\n",
       " 'body',\n",
       " 'pussy',\n",
       " 'wit',\n",
       " 'check',\n",
       " 'everybody',\n",
       " 'lot',\n",
       " 'lost',\n",
       " 'change',\n",
       " 'hope',\n",
       " 'drop',\n",
       " 'dead',\n",
       " 'yuh',\n",
       " 'mean',\n",
       " 'start',\n",
       " 'end',\n",
       " 'coming',\n",
       " 'hey',\n",
       " 'wait',\n",
       " 'till',\n",
       " 'bag',\n",
       " 'crazy',\n",
       " 'used',\n",
       " 'ooh',\n",
       " 'living',\n",
       " 'trying',\n",
       " 'care',\n",
       " 'pop',\n",
       " 'flow',\n",
       " 'light',\n",
       " 'break',\n",
       " 'friends',\n",
       " 'making',\n",
       " 'inside',\n",
       " 'mad',\n",
       " 'bring',\n",
       " 'cold',\n",
       " 'walk',\n",
       " 'city',\n",
       " 'old',\n",
       " 'took',\n",
       " 'roll',\n",
       " 'catch',\n",
       " 'soul',\n",
       " 'believe',\n",
       " 'straight',\n",
       " 'nah',\n",
       " 'hot',\n",
       " 'imma',\n",
       " 'seen',\n",
       " 'free',\n",
       " 'fall',\n",
       " 'hook',\n",
       " 'different',\n",
       " 'place',\n",
       " 'days',\n",
       " 'gettin',\n",
       " 'til',\n",
       " 'broke',\n",
       " 'block',\n",
       " 'round',\n",
       " 'music',\n",
       " 'fly',\n",
       " 'wrong',\n",
       " 'cash',\n",
       " 'hell',\n",
       " 'white',\n",
       " 'phone',\n",
       " 'running',\n",
       " 'blow',\n",
       " 'okay',\n",
       " 'guess',\n",
       " 'chorus',\n",
       " 'heard',\n",
       " 'dont',\n",
       " 'trust',\n",
       " 'low',\n",
       " 'ready',\n",
       " 'swear',\n",
       " 'ask',\n",
       " 'times',\n",
       " 'ay',\n",
       " 'rock',\n",
       " 'da',\n",
       " 'woah',\n",
       " 'fucked',\n",
       " 'cut',\n",
       " 'maybe',\n",
       " 'listen',\n",
       " 'line',\n",
       " 'house',\n",
       " 'remember',\n",
       " 'hand',\n",
       " 'fake',\n",
       " 'eat',\n",
       " 'pay',\n",
       " 'shoot',\n",
       " 'ah',\n",
       " 'throw',\n",
       " 'fast',\n",
       " 'hands',\n",
       " 'talkin',\n",
       " 'shot',\n",
       " 'went',\n",
       " 'thinking',\n",
       " 'streets',\n",
       " 'sleep',\n",
       " 'shawty',\n",
       " 'bro',\n",
       " 'dope',\n",
       " 'fight',\n",
       " 'song',\n",
       " 'couple',\n",
       " 'wish',\n",
       " 'hoe',\n",
       " 'gave',\n",
       " 'deep',\n",
       " 'goin',\n",
       " 'girls',\n",
       " 'knew',\n",
       " 'lose',\n",
       " 'lie',\n",
       " 'sick',\n",
       " 'hop',\n",
       " 'past',\n",
       " 'kid',\n",
       " 'king',\n",
       " 'dream',\n",
       " 'ha',\n",
       " 'help',\n",
       " 'red',\n",
       " 'love',\n",
       " 'know',\n",
       " 'oh',\n",
       " 'like',\n",
       " 'na',\n",
       " 'got',\n",
       " 'time',\n",
       " 'yeah',\n",
       " 'let',\n",
       " 'want',\n",
       " 'come',\n",
       " 'baby',\n",
       " 'say',\n",
       " 'way',\n",
       " 'feel',\n",
       " 'make',\n",
       " 'cause',\n",
       " 'wan',\n",
       " 'life',\n",
       " 'need',\n",
       " 'away',\n",
       " 'gon',\n",
       " 'heart',\n",
       " 'right',\n",
       " 'night',\n",
       " 'tell',\n",
       " 'day',\n",
       " 'think',\n",
       " 'world',\n",
       " 'eyes',\n",
       " 'good',\n",
       " 'mind',\n",
       " 'girl',\n",
       " 'man',\n",
       " 'said',\n",
       " 'ooh',\n",
       " 'look',\n",
       " 'little',\n",
       " 'la',\n",
       " 'home',\n",
       " 'long',\n",
       " 'things',\n",
       " 'better',\n",
       " 'hold',\n",
       " 'hey',\n",
       " 'stay',\n",
       " 'light',\n",
       " 'really',\n",
       " 'leave',\n",
       " 'try',\n",
       " 'gone',\n",
       " 'tonight',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'stop',\n",
       " 'new',\n",
       " 'run',\n",
       " 'fall',\n",
       " 'inside',\n",
       " 'believe',\n",
       " 'going',\n",
       " 'feeling',\n",
       " 'ta',\n",
       " 'live',\n",
       " 'left',\n",
       " 'god',\n",
       " 'turn',\n",
       " 'face',\n",
       " 'place',\n",
       " 'ah',\n",
       " 'lost',\n",
       " 'thing',\n",
       " 'end',\n",
       " 'people',\n",
       " 'hard',\n",
       " 'sun',\n",
       " 'free',\n",
       " 'maybe',\n",
       " 'bad',\n",
       " 'ya',\n",
       " 'wrong',\n",
       " 'thought',\n",
       " 'high',\n",
       " 'change',\n",
       " 'dream',\n",
       " 'break',\n",
       " 'soul',\n",
       " 'hand',\n",
       " 'forever',\n",
       " 'old',\n",
       " 'dance',\n",
       " 'die',\n",
       " 'far',\n",
       " 'hope',\n",
       " 'play',\n",
       " 'care',\n",
       " 'true',\n",
       " 'real',\n",
       " 'boy',\n",
       " 'days',\n",
       " 'sky',\n",
       " 'pain',\n",
       " 'walk',\n",
       " 'body',\n",
       " 'chorus',\n",
       " 'talk',\n",
       " 'looking',\n",
       " 'waiting',\n",
       " 'words',\n",
       " 'coming',\n",
       " 'hands',\n",
       " 'song',\n",
       " 'wait',\n",
       " 'wish',\n",
       " 'sing',\n",
       " 'remember',\n",
       " 'start',\n",
       " 'close',\n",
       " 'dreams',\n",
       " 'best',\n",
       " 'da',\n",
       " 'money',\n",
       " 'stand',\n",
       " 'cold',\n",
       " 'yes',\n",
       " 'told',\n",
       " 'friends',\n",
       " 'fight',\n",
       " 'comes',\n",
       " 'help',\n",
       " 'falling',\n",
       " 'living',\n",
       " 'forget',\n",
       " 'fuck',\n",
       " 'bring',\n",
       " 'alright',\n",
       " 'knew',\n",
       " 'open',\n",
       " 'running',\n",
       " 'lord',\n",
       " 'sweet',\n",
       " 'everybody',\n",
       " 'rain',\n",
       " 'lie',\n",
       " 'getting',\n",
       " 'dark',\n",
       " 'trying',\n",
       " 'big',\n",
       " 'blue',\n",
       " 'watch',\n",
       " 'lose',\n",
       " 'shit',\n",
       " 'came',\n",
       " 'fly',\n",
       " 'uh',\n",
       " 'times',\n",
       " 'touch',\n",
       " 'kiss',\n",
       " 'round',\n",
       " 'till',\n",
       " 'friend',\n",
       " 'feels',\n",
       " 'deep',\n",
       " 'alive',\n",
       " 'used',\n",
       " 'sleep',\n",
       " 'fine',\n",
       " 'smile',\n",
       " 'door',\n",
       " 'crazy',\n",
       " 'makes',\n",
       " 'today',\n",
       " 'hate',\n",
       " 'dead',\n",
       " 'miss',\n",
       " 'work',\n",
       " 'black',\n",
       " 'morning',\n",
       " 'mean',\n",
       " 'goes',\n",
       " 'sound',\n",
       " 'seen',\n",
       " 'lonely',\n",
       " 'ride',\n",
       " 'broken',\n",
       " 'sure',\n",
       " 'guess',\n",
       " 'blood',\n",
       " 'truth',\n",
       " 'thinking',\n",
       " 'took',\n",
       " 'stars',\n",
       " 'em',\n",
       " 'happy',\n",
       " 'ground',\n",
       " 'matter',\n",
       " 'hit',\n",
       " 'lies',\n",
       " 'ready',\n",
       " 'tears',\n",
       " 'line',\n",
       " 'saw',\n",
       " 'late',\n",
       " 'fear',\n",
       " 'hide',\n",
       " 'hell',\n",
       " 'beautiful',\n",
       " 'town',\n",
       " 'lights',\n",
       " 'rock',\n",
       " 'understand',\n",
       " 'young',\n",
       " 'wanted',\n",
       " 'years',\n",
       " 'save',\n",
       " 'somebody',\n",
       " 'knows',\n",
       " 'easy',\n",
       " 'heaven',\n",
       " 'goodbye',\n",
       " 'hurt',\n",
       " 'set',\n",
       " 'road',\n",
       " 'wake',\n",
       " 'bout',\n",
       " 'til',\n",
       " 'control',\n",
       " 'party',\n",
       " 'water',\n",
       " 'arms',\n",
       " 'past',\n",
       " 'listen',\n",
       " 'room',\n",
       " 'kind',\n",
       " 'shine',\n",
       " 'chance',\n",
       " 'beat',\n",
       " 'tried',\n",
       " 'ask',\n",
       " 'music',\n",
       " 'anymore',\n",
       " 'pretty',\n",
       " 'gave',\n",
       " 'game',\n",
       " 'slow',\n",
       " 'follow',\n",
       " 'wonder',\n",
       " 'heard',\n",
       " 'bed',\n",
       " 'feet',\n",
       " 'love',\n",
       " 'know',\n",
       " 'like',\n",
       " 'got',\n",
       " 'na',\n",
       " 'oh',\n",
       " 'time',\n",
       " 'way',\n",
       " 'yeah',\n",
       " 'let',\n",
       " 'gon',\n",
       " 'cause',\n",
       " 'baby',\n",
       " 'little',\n",
       " 'heart',\n",
       " 'say',\n",
       " 'come',\n",
       " 'night',\n",
       " 'good',\n",
       " 'man',\n",
       " 'away',\n",
       " 'want',\n",
       " 'home',\n",
       " 'make',\n",
       " 'old',\n",
       " 'right',\n",
       " 'day',\n",
       " 'long',\n",
       " 'said',\n",
       " 'life',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'need',\n",
       " 'nerves',\n",
       " 'girl',\n",
       " 'wan',\n",
       " 'song',\n",
       " 'feel',\n",
       " 'world',\n",
       " 'gone',\n",
       " 'look',\n",
       " 'mind',\n",
       " 'eyes',\n",
       " 'left',\n",
       " 'town',\n",
       " 'things',\n",
       " 'hold',\n",
       " 'better',\n",
       " 'thing',\n",
       " 'road',\n",
       " 'big',\n",
       " 'leave',\n",
       " 'hard',\n",
       " 'tonight',\n",
       " 'lord',\n",
       " 'light',\n",
       " 'run',\n",
       " 'hear',\n",
       " 'new',\n",
       " 'ooh',\n",
       " 'stay',\n",
       " 'god',\n",
       " 'blue',\n",
       " 'country',\n",
       " 'boy',\n",
       " 'hand',\n",
       " 'believe',\n",
       " 'fall',\n",
       " 'live',\n",
       " 'wrong',\n",
       " 'thought',\n",
       " 'place',\n",
       " 'head',\n",
       " 'going',\n",
       " 'turn',\n",
       " 'sweet',\n",
       " 'days',\n",
       " 'sun',\n",
       " 'ta',\n",
       " 'lost',\n",
       " 'hey',\n",
       " 'told',\n",
       " 'wish',\n",
       " 'cold',\n",
       " 'really',\n",
       " 'people',\n",
       " 'high',\n",
       " 'end',\n",
       " 'rain',\n",
       " 'try',\n",
       " 'change',\n",
       " 'crazy',\n",
       " 'sing',\n",
       " 'walk',\n",
       " 'em',\n",
       " 'maybe',\n",
       " 'took',\n",
       " 'soul',\n",
       " 'hell',\n",
       " 'free',\n",
       " 'came',\n",
       " 'true',\n",
       " 'best',\n",
       " 'bad',\n",
       " 'used',\n",
       " 'drink',\n",
       " 'hands',\n",
       " 'woman',\n",
       " 'face',\n",
       " 'line',\n",
       " 'comes',\n",
       " 'sure',\n",
       " 'goes',\n",
       " 'red',\n",
       " 'wind',\n",
       " 'hope',\n",
       " 'door',\n",
       " 'kiss',\n",
       " 'morning',\n",
       " 'til',\n",
       " 'knew',\n",
       " 'coming',\n",
       " 'far',\n",
       " 'break',\n",
       " 'friends',\n",
       " 'lonely',\n",
       " 'times',\n",
       " 'dream',\n",
       " 'guess',\n",
       " 'years',\n",
       " 'yes',\n",
       " 'ride',\n",
       " 'stop',\n",
       " 'seen',\n",
       " 'pretty',\n",
       " 'ground',\n",
       " 'start',\n",
       " 'daddy',\n",
       " 'saw',\n",
       " 'close',\n",
       " 'play',\n",
       " 'round',\n",
       " 'help',\n",
       " 'die',\n",
       " 'sky',\n",
       " 'smile',\n",
       " 'miss',\n",
       " 'friend',\n",
       " 'knows',\n",
       " 'remember',\n",
       " 'water',\n",
       " 'dark',\n",
       " 'everybody',\n",
       " 'looking',\n",
       " 'makes',\n",
       " 'bout',\n",
       " 'lay',\n",
       " 'heaven',\n",
       " 'real',\n",
       " 'went',\n",
       " 'honey',\n",
       " 'mama',\n",
       " 'forever',\n",
       " 'goodbye',\n",
       " 'deep',\n",
       " 'river',\n",
       " 'today',\n",
       " 'feeling',\n",
       " 'black',\n",
       " 'roll',\n",
       " 'till',\n",
       " 'words',\n",
       " 'dreams',\n",
       " 'lights',\n",
       " 'fly',\n",
       " 'arms',\n",
       " 'white',\n",
       " 'inside',\n",
       " 'tears',\n",
       " 'bring',\n",
       " 'strong',\n",
       " 'wonder',\n",
       " 'happy',\n",
       " 'dance',\n",
       " 'boys',\n",
       " 'somebody',\n",
       " 'work',\n",
       " 'care',\n",
       " 'money',\n",
       " 'pain',\n",
       " 'living',\n",
       " 'wait',\n",
       " 'fine',\n",
       " 'train',\n",
       " 'stars',\n",
       " 'heard',\n",
       " 'moon',\n",
       " 'loved',\n",
       " 'talk',\n",
       " 'damn',\n",
       " 'ya',\n",
       " 'la',\n",
       " 'lot',\n",
       " 'city',\n",
       " 'jesus',\n",
       " 'ah',\n",
       " 'getting',\n",
       " 'stand',\n",
       " 'young',\n",
       " 'matter',\n",
       " 'anymore',\n",
       " 'fool',\n",
       " 'late',\n",
       " 'truck',\n",
       " 'born',\n",
       " 'easy',\n",
       " 'forget',\n",
       " 'kind',\n",
       " 'soon',\n",
       " 'mean',\n",
       " 'lose',\n",
       " 'waiting',\n",
       " 'fight',\n",
       " 'open',\n",
       " 'beer',\n",
       " 'running',\n",
       " 'broken',\n",
       " 'hurt',\n",
       " 'called',\n",
       " 'feet',\n",
       " 'gets',\n",
       " 'house',\n",
       " 'rock',\n",
       " 'whiskey',\n",
       " 'land',\n",
       " 'christmas',\n",
       " 'livin',\n",
       " 'sleep',\n",
       " 'hair',\n",
       " 'drive',\n",
       " 'watch',\n",
       " 'reason',\n",
       " 'mountain',\n",
       " 'chorus',\n",
       " 'nothin',\n",
       " 'wild',\n",
       " 'summer',\n",
       " 'gave',\n",
       " 'slow',\n",
       " 'son',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = []\n",
    "for tag in data[\"tag\"].unique():\n",
    "    tag_top_words = top_words_per_tag(dataset=data,column='clean_lyrics',label=tag,k = 250)\n",
    "    top_words.extend(tag_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.925600Z",
     "start_time": "2023-12-21T12:15:11.925579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique top words: 461\n",
      "Total number of top_words across all 6 tags:117\n",
      "Total number of top_words only in 1 tag:151\n"
     ]
    }
   ],
   "source": [
    "value_counts = Counter(top_words)\n",
    "common_words = {word: count for word, count in value_counts.items() if count >= 6}\n",
    "unique_words =  {word: count for word, count in value_counts.items() if count == 1}\n",
    "common_words = list(common_words.keys())\n",
    "print(f'Unique top words: {len(value_counts)}')\n",
    "print(f'Total number of top_words across all 6 tags:{len(common_words)}')\n",
    "print(f'Total number of top_words only in 1 tag:{len(unique_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.927748Z",
     "start_time": "2023-12-21T12:15:11.927725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(common_words)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class='anchor' id='5'></a>\n",
    "<font color='#FE7845'><font><h2>4. Vectorization</h2></font>\n",
    "[Top &#129033;](#0) \n",
    "    \n",
    "Selecting the column we want to use for Vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.933215Z",
     "start_time": "2023-12-21T12:15:11.933191Z"
    }
   },
   "outputs": [],
   "source": [
    "column = data['lemmatized_lyrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='6'></a>\n",
    "<font color='#FE7845'><font><h3>4.1 Bag of Words (BoW)</h3></font>\n",
    "[Top &#129033;](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.935197Z",
     "start_time": "2023-12-21T12:15:11.935178Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(stop_words=stop_words,max_features=2500)\n",
    "# corpus = column\n",
    "# X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# print(\"Selected Features:\", feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='7'></a>\n",
    "<font color='#FE7845'><font><h3>4.2. TF-IDF</h3></font>\n",
    "[Top &#129033;](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.937049Z",
     "start_time": "2023-12-21T12:15:11.937027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['aah' 'able' 'accept' ... 'yup' 'zero' 'zone']\n"
     ]
    }
   ],
   "source": [
    "# vectorizer = TfidfVectorizer(stop_words=stop_words,max_features=2500)\n",
    "# corpus = column\n",
    "# X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# print(\"Selected Features:\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='8'></a>\n",
    "<font color='#FE7845'><font><h3>4.3. Document embeddings (Doc2Vec)</h3></font>\n",
    "[Top &#129033;](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.939175Z",
     "start_time": "2023-12-21T12:15:11.939152Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = list(read_corpus(column=column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model and building its vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.941618Z",
     "start_time": "2023-12-21T12:15:11.941597Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=300,negative=5, hs=0, min_count=2,dm=0, sample = 0,epochs=30,workers = 8)\n",
    "model.build_vocab(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20041937"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.943513Z",
     "start_time": "2023-12-21T12:15:11.943494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'text' appeared 2695 times in the training corpus.\n",
      "Word 'mine' appeared 19593 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'text' appeared {model.wv.get_vecattr('text', 'count')} times in the training corpus.\")\n",
    "print(f\"Word 'mine' appeared {model.wv.get_vecattr('mine', 'count')} times in the training corpus.\") # mining turns into mine when lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.945575Z",
     "start_time": "2023-12-21T12:15:11.945556Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we will select a document at random and compare it to the most and least similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.948402Z",
     "start_time": "2023-12-21T12:15:11.948381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (114468) - rb: «unforgettable incredible metaphor ready intellectual electrical glow like eddison ahead em head spectacle could weapon could cause load deadly eardrummers undeniable unbelievable undefeatable pretty pow rful high unreachable impeachabl promise never forget like obama never forget like alzheimer memory lane driver lil wayne timer rather forget remember wrong nat king cole call nat king kong take coat take cliche show side one see get whole world right pocket chase stardust melody wan na leave alone get ta put fine wan na sing song get ta put time timeless wan na take let chandelier fall fall stand tall like taj mahal unforgettable like nat king cole like nat king cole end yesterday hit chord everyone hear emerald ruby black white movie never truly fade away cause greatness never disappear wan na leave alone get ta put fine wan na sing song get ta put time timeless wan na take let chandelier fall fall stand tall like taj mahal unforgettable like nat king cole take moment storm like nat king cole oh yeah like nat king cole unforgettable like nat king cole oh yeah oh yeah oh yeah yeah show show mm mm mm mm show show mm mm mm mm mike make»\n",
      "Most similar document (36873, 0.37434226274490356) - pop: «melody head forget air breathe stand edge never alone never alone never alone never alone believe believe never alone never alone melody head forget air breathe stand edge never alone never alone never alone never alone believe hey believe never alone never alone»\n",
      "Least similar document (53584, -0.11838408559560776) - misc: «june th dear barbara alexievna away melancholy really beloved ought ashamed allow thought enter head really truly quite well really truly darling bloom simply bloom true see certain touch pallor face still bloom fig dream vision yes shame dear drive away fancy try despise sleep well never ail look beloved live well sleep peacefully retain health ruffle junior fact pleasure see come come sweetheart let u know little head capable fancy easily take dream repine sake cease go people ask never could think thing take journey allow intend combat intention might sell frockcoat walk street shirt sleeve rather let want barbara know know merely trick merely trick probably thedora alone blame appear foolish old woman able persuade anything believe dear sure know well eh sweetheart stupid quarrelsome rubbish talk old woman bring late husband grave probably plague much dear must take step would leave pray put idea head lack feel sufficiently overjoy near part love well live life quietly wish read sew whichever like read sew desert try imagine thing would seem go send book late go walk come come barbara summon aid reason cease babble trifle soon come see shall tell whole story sweetheart certainly course know educate man receive sorry school inclination think much rataziaev friend therefore must put word two yes splendid writer assert write magnificently agree work never shall write ornately laconically great wealth imagery imagination perhaps read without insight barbara perhaps spirit time angry thedora something worry mischance ah read sympathetically best time feel happy content pleasantly dispose instance bonbon two mouth yes way read rataziaev dispute indeed would good writer exist even far good good good write well write well chiefly sake write approve goodbye dear write must hurry away business good cheer lord god watch faithful friend makar dievushkin thank much book darling read volume pushkin tonight come dear makar alexievitch friend must go live near think matter come conclusion wrong refuse good post least assure crust bread might least set work earn employer favour even try change character require course sad sorry thing live among strange force seek patronage conceal constrain one personality god help must remain forever recluse similar chance come way remember little girl school use go home sunday spend time frisk dance sometimes mother would chide care heart joyous spirit buoyant yet even sunday come sadness death would overtake nine clock return school everything cold strange severe govern monday lose temper nip ear make cry occasion would retire corner weep alone conceal tear l call lazy yet study use weep time grow use thing schooldays shed tear part friend right live dependence upon think torture tell frankly reason frankness become habit see daily early dawn thedora rise wash scrub remain work late night even though poor old bone must ache want rest also see ruin hoard last kopeck may spend behalf ought act friend even though write would rather sell let want anything believe friend entirely believe good heart say perhaps receive unexpected sum gratuity still future think know always ail work glad though work could get else sit repine watch thedora would use necessary comrade mine ever do good though bind whole soul love dearly strongly wholeheartedly bitter fate ordain love give unable create subsistence repay kindness therefore detain long think matter give opinion expectation remain sweetheart»\n"
     ]
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(corpus) - 1) # selecting a random document\n",
    "inferred_vector = model.infer_vector(corpus[doc_id].words) # getting its vector representation\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv)) # list of most similar documents\n",
    "\n",
    "# Compare and print the most/least similar documents from the train corpus\n",
    "print(f\"Test Document ({doc_id}) - {data.iloc[doc_id].tag}: «{' '.join(corpus[doc_id].words)}»\")\n",
    "print(f\"Most similar document {sims[1]} - {data.iloc[sims[1][0]].tag}: «{' '.join(corpus[sims[1][0]].words)}»\")\n",
    "print(f\"Least similar document {sims[len(sims) - 1]} - {data.iloc[sims[len(sims) - 1][0]].tag}: «{' '.join(corpus[sims[len(sims) - 1][0]].words)}»\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the document vectors as X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/133905 [00:00<?, ?document/s]Processing: 100%|██████████| 133905/133905 [00:00<00:00, 372998.18document/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for i in range(len(model.dv)):\n",
    "    X.append(model.dv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133905"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class='anchor' id='9'></a>\n",
    "<font color='#FE7845'><font><h2>5. Model Training</h2></font>\n",
    "[Top &#129033;](#0) \n",
    "    \n",
    "Defining the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.953018Z",
     "start_time": "2023-12-21T12:15:11.952999Z"
    }
   },
   "outputs": [],
   "source": [
    "y = data['tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='10'></a>\n",
    "<font color='#FE7845'><font><h3>5.1. Logistic Regression</h3></font>\n",
    "[Top &#129033;](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver_penalty_grid = [\n",
    "#     {'solver': [['lbfgs']], 'penalty': [['l2']]},\n",
    "#     {'solver': [['liblinear']], 'penalty': [['l1', 'l2']]},\n",
    "#     {'solver': [['newton-cg']], 'penalty': [['l2']]},\n",
    "#     {'solver': [['newton-cholesky']], 'penalty': [['l2']]},\n",
    "#     {'solver': [['sag']], 'penalty': [['l2']]},\n",
    "#     {'solver': [['saga']], 'penalty': [['elasticnet', 'l1', 'l2', None]]}\n",
    "# ]\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'C': [[.25, 0.5, 0.75, 1]],\n",
    "#     'warm_start': [[True, False]]\n",
    "# }\n",
    "\n",
    "# # Extend each dictionary in solver_penalty_grid with the parameters from param_grid\n",
    "# extended_grid = [{**params, **param_grid} for params in solver_penalty_grid]\n",
    "\n",
    "# # Create the parameter grid iterator\n",
    "# grid = list(ParameterGrid(extended_grid))\n",
    "\n",
    "# clf = GridSearchCV(estimator=LogisticRegression(random_state=42), param_grid=grid, scoring='f1_weighted',verbose=3)\n",
    "\n",
    "# clf.fit(X, y)\n",
    "# best_params = clf.best_params_    #{'C': 0.25, 'class_weight': None, 'penalty': None, 'solver': 'saga', 'warm_start': True}\n",
    "# print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified k-fold for logistic regression using parameters from gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.955230Z",
     "start_time": "2023-12-21T12:15:11.955211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.48      0.12      0.19       683\n",
      "        misc       0.74      0.41      0.53       960\n",
      "         pop       0.60      0.83      0.70     11127\n",
      "         rap       0.83      0.87      0.85      7710\n",
      "          rb       0.45      0.11      0.17      1236\n",
      "        rock       0.54      0.25      0.34      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.61      0.43      0.46     26781\n",
      "weighted avg       0.65      0.67      0.63     26781\n",
      "\n",
      "\n",
      "\n",
      "Fold 2:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.48      0.12      0.19       682\n",
      "        misc       0.70      0.38      0.49       960\n",
      "         pop       0.61      0.84      0.70     11127\n",
      "         rap       0.82      0.87      0.85      7710\n",
      "          rb       0.45      0.11      0.17      1237\n",
      "        rock       0.55      0.25      0.34      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.60      0.43      0.46     26781\n",
      "weighted avg       0.65      0.67      0.63     26781\n",
      "\n",
      "\n",
      "\n",
      "Fold 3:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.52      0.14      0.22       682\n",
      "        misc       0.72      0.40      0.51       960\n",
      "         pop       0.60      0.83      0.70     11127\n",
      "         rap       0.83      0.86      0.85      7710\n",
      "          rb       0.42      0.10      0.17      1237\n",
      "        rock       0.53      0.25      0.34      5065\n",
      "\n",
      "    accuracy                           0.66     26781\n",
      "   macro avg       0.60      0.43      0.46     26781\n",
      "weighted avg       0.65      0.66      0.63     26781\n",
      "\n",
      "\n",
      "\n",
      "Fold 4:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.54      0.12      0.20       682\n",
      "        misc       0.72      0.39      0.51       960\n",
      "         pop       0.60      0.83      0.70     11127\n",
      "         rap       0.82      0.87      0.85      7710\n",
      "          rb       0.40      0.11      0.17      1237\n",
      "        rock       0.53      0.25      0.34      5065\n",
      "\n",
      "    accuracy                           0.66     26781\n",
      "   macro avg       0.60      0.43      0.46     26781\n",
      "weighted avg       0.64      0.66      0.63     26781\n",
      "\n",
      "\n",
      "\n",
      "Fold 5:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.44      0.10      0.17       682\n",
      "        misc       0.69      0.38      0.49       961\n",
      "         pop       0.60      0.83      0.70     11127\n",
      "         rap       0.82      0.88      0.85      7710\n",
      "          rb       0.40      0.10      0.16      1236\n",
      "        rock       0.55      0.25      0.34      5065\n",
      "\n",
      "    accuracy                           0.66     26781\n",
      "   macro avg       0.58      0.42      0.45     26781\n",
      "weighted avg       0.65      0.66      0.63     26781\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the folds\n",
    "#for fold, (train_idx, val_idx) in enumerate(stratified_kfold.split(X, y)):\n",
    "    # Extract the training and testing data for this fold\n",
    "#    try:\n",
    "#        X_train, X_val = X[train_idx], X[val_idx]\n",
    "#        y_train, y_val = y[train_idx], y[val_idx]\n",
    "#    except:\n",
    "#        X = np.asarray(X)\n",
    "#        X_train, X_val = X[train_idx], X[val_idx]\n",
    "#        y_train, y_val = y[train_idx], y[val_idx]\n",
    "#\n",
    "    # Now you can train and evaluate your model on this fold\n",
    "#    print(f\"Fold {fold + 1}:\")\n",
    "#    print(\"Training data:\", X_train.shape[0], len(y_train))\n",
    "#    print(\"Testing data:\", X_val.shape[0], len(y_val))\n",
    "    # Your model training and evaluation code goes here\n",
    "#    best_params = {'C': 0.25, 'class_weight': None, 'penalty': None, 'solver': 'saga', 'warm_start': True}\n",
    "#    lr = LogisticRegression(**best_params)\n",
    "#    lr.fit(X_train,y_train)\n",
    "#    predictions = lr.predict(X_val)\n",
    "#    print(classification_report(y_val, predictions))\n",
    "#    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='11'></a>\n",
    "<font color='#FE7845'><font><h3>5.2. MultinomialNB</h3></font>\n",
    "[Top &#129033;](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB only works when X doesn't include negative values. Therefore it wasn't applied when X resulted from the document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.959986Z",
     "start_time": "2023-12-21T12:15:11.959963Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'alpha': [0,.25, 0.5, 0.75, 1.0],\n",
    "#     'fit_prior': [True, False],\n",
    "#     'force_alpha': [True, False]\n",
    "# }\n",
    "\n",
    "# clf = GridSearchCV(estimator=MultinomialNB(), param_grid=param_grid, scoring='f1_weighted',verbose=3)\n",
    "\n",
    "# clf.fit(X, y)\n",
    "# best_params = clf.best_params_\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.961726Z",
     "start_time": "2023-12-21T12:15:11.961702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      3 ... 133902 133903 133904]\n",
      "Fold 1:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.75      0.00      0.01       683\n",
      "        misc       0.85      0.25      0.38       960\n",
      "         pop       0.56      0.87      0.68     11127\n",
      "         rap       0.78      0.79      0.79      7710\n",
      "          rb       0.00      0.00      0.00      1236\n",
      "        rock       0.56      0.14      0.22      5065\n",
      "\n",
      "    accuracy                           0.63     26781\n",
      "   macro avg       0.58      0.34      0.35     26781\n",
      "weighted avg       0.61      0.63      0.56     26781\n",
      "\n",
      "\n",
      "\n",
      "[     2      3      5 ... 133901 133902 133904]\n",
      "Fold 2:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.75      0.00      0.01       682\n",
      "        misc       0.85      0.24      0.37       960\n",
      "         pop       0.56      0.88      0.68     11127\n",
      "         rap       0.78      0.79      0.79      7710\n",
      "          rb       0.25      0.00      0.00      1237\n",
      "        rock       0.55      0.13      0.22      5065\n",
      "\n",
      "    accuracy                           0.63     26781\n",
      "   macro avg       0.62      0.34      0.34     26781\n",
      "weighted avg       0.62      0.63      0.56     26781\n",
      "\n",
      "\n",
      "\n",
      "[     0      1      2 ... 133901 133902 133903]\n",
      "Fold 3:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.33      0.00      0.00       682\n",
      "        misc       0.86      0.25      0.39       960\n",
      "         pop       0.56      0.88      0.68     11127\n",
      "         rap       0.79      0.80      0.79      7710\n",
      "          rb       0.20      0.00      0.00      1237\n",
      "        rock       0.57      0.13      0.22      5065\n",
      "\n",
      "    accuracy                           0.63     26781\n",
      "   macro avg       0.55      0.34      0.35     26781\n",
      "weighted avg       0.62      0.63      0.57     26781\n",
      "\n",
      "\n",
      "\n",
      "[     0      1      2 ... 133902 133903 133904]\n",
      "Fold 4:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.80      0.01      0.01       682\n",
      "        misc       0.82      0.24      0.37       960\n",
      "         pop       0.56      0.88      0.68     11127\n",
      "         rap       0.79      0.80      0.79      7710\n",
      "          rb       0.00      0.00      0.00      1237\n",
      "        rock       0.57      0.14      0.23      5065\n",
      "\n",
      "    accuracy                           0.63     26781\n",
      "   macro avg       0.59      0.34      0.35     26781\n",
      "weighted avg       0.62      0.63      0.57     26781\n",
      "\n",
      "\n",
      "\n",
      "[     0      1      2 ... 133901 133903 133904]\n",
      "Fold 5:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.67      0.00      0.01       682\n",
      "        misc       0.85      0.25      0.39       961\n",
      "         pop       0.56      0.88      0.68     11127\n",
      "         rap       0.79      0.80      0.79      7710\n",
      "          rb       0.00      0.00      0.00      1236\n",
      "        rock       0.56      0.13      0.22      5065\n",
      "\n",
      "    accuracy                           0.63     26781\n",
      "   macro avg       0.57      0.34      0.35     26781\n",
      "weighted avg       0.61      0.63      0.57     26781\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Iterate over the folds\n",
    "# for fold, (train_idx, val_idx) in enumerate(stratified_kfold.split(X, y)):\n",
    "#     # Extract the training and testing data for this fold\n",
    "#     X_train, X_val = X[train_idx], X[val_idx]\n",
    "#     y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "#     # Now you can train and evaluate your model on this fold\n",
    "#     print(f\"Fold {fold + 1}:\")\n",
    "#     print(\"Training data:\", X_train.shape[0], len(y_train))\n",
    "#     print(\"Testing data:\", X_val.shape[0], len(y_val))\n",
    "#     # Your model training and evaluation code goes here\n",
    "#     mnb = MultinomialNB(**best_params)\n",
    "#     mnb.fit(X_train,y_train)\n",
    "#     predictions = mnb.predict(X_val)\n",
    "#     print(classification_report(y_val, predictions))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class='anchor' id='12'></a>\n",
    "<font color='#FE7845'><font><h3>5.3. Neural Networks</h3></font>\n",
    "[Top &#129033;](#0)\n",
    "    \n",
    "<font color='#FF914D'><font>**MultiLayerPerceptron** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.963647Z",
     "start_time": "2023-12-21T12:15:11.963626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV 1/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.638 total time=   8.4s\n",
      "[CV 2/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.643 total time=   6.9s\n",
      "[CV 3/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.622 total time=   7.0s\n",
      "[CV 4/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.635 total time=   7.0s\n",
      "[CV 5/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.628 total time=   7.2s\n",
      "[CV 1/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.638 total time=   7.9s\n",
      "[CV 2/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.643 total time=   7.8s\n",
      "[CV 3/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.622 total time=   7.0s\n",
      "[CV 4/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.635 total time=   8.1s\n",
      "[CV 5/5] END activation=relu, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.628 total time=   7.8s\n",
      "[CV 1/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.633 total time=  10.1s\n",
      "[CV 2/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.641 total time=  10.5s\n",
      "[CV 3/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.625 total time=   9.9s\n",
      "[CV 4/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.629 total time=   9.8s\n",
      "[CV 5/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.634 total time=  10.0s\n",
      "[CV 1/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.633 total time=  10.2s\n",
      "[CV 2/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.641 total time=  10.7s\n",
      "[CV 3/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.625 total time=  10.1s\n",
      "[CV 4/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.629 total time=  10.0s\n",
      "[CV 5/5] END activation=relu, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.634 total time=  10.0s\n",
      "[CV 1/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.632 total time=   6.3s\n",
      "[CV 2/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.643 total time=   7.7s\n",
      "[CV 3/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.633 total time=   6.8s\n",
      "[CV 4/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.628 total time=   8.8s\n",
      "[CV 5/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.632 total time=   7.7s\n",
      "[CV 1/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.632 total time=   7.0s\n",
      "[CV 2/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.643 total time=   8.2s\n",
      "[CV 3/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.633 total time=   7.5s\n",
      "[CV 4/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.628 total time=   9.2s\n",
      "[CV 5/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.632 total time=   8.0s\n",
      "[CV 1/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.636 total time=  10.6s\n",
      "[CV 2/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.635 total time=   8.2s\n",
      "[CV 3/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.635 total time=   8.7s\n",
      "[CV 4/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.639 total time=   8.2s\n",
      "[CV 5/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.637 total time=   8.5s\n",
      "[CV 1/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.636 total time=   9.7s\n",
      "[CV 2/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.635 total time=   7.8s\n",
      "[CV 3/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.635 total time=   8.3s\n",
      "[CV 4/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.639 total time=   8.5s\n",
      "[CV 5/5] END activation=relu, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.637 total time=   8.3s\n",
      "[CV 1/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.635 total time=  10.8s\n",
      "[CV 2/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.643 total time=  12.2s\n",
      "[CV 3/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.626 total time=  11.2s\n",
      "[CV 4/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.637 total time=   9.2s\n",
      "[CV 5/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.631 total time=  13.1s\n",
      "[CV 1/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.635 total time=  10.2s\n",
      "[CV 2/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.643 total time=  11.5s\n",
      "[CV 3/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.626 total time=   9.9s\n",
      "[CV 4/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.637 total time=   8.2s\n",
      "[CV 5/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.631 total time=  12.0s\n",
      "[CV 1/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.637 total time=  10.3s\n",
      "[CV 2/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.641 total time=  11.8s\n",
      "[CV 3/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.633 total time=  12.4s\n",
      "[CV 4/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.635 total time=  10.5s\n",
      "[CV 5/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.634 total time=  10.5s\n",
      "[CV 1/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.637 total time=  10.3s\n",
      "[CV 2/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.641 total time=  12.0s\n",
      "[CV 3/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.633 total time=  11.3s\n",
      "[CV 4/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.635 total time=  10.0s\n",
      "[CV 5/5] END activation=relu, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.634 total time=  10.3s\n",
      "[CV 1/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.640 total time=  10.2s\n",
      "[CV 2/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.644 total time=  10.3s\n",
      "[CV 3/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.635 total time=   9.1s\n",
      "[CV 4/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.644 total time=   9.6s\n",
      "[CV 5/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.635 total time=   9.9s\n",
      "[CV 1/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.640 total time=   9.6s\n",
      "[CV 2/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.644 total time=  10.0s\n",
      "[CV 3/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.635 total time=   9.1s\n",
      "[CV 4/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.644 total time=   8.9s\n",
      "[CV 5/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.635 total time=  11.1s\n",
      "[CV 1/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.639 total time=  12.6s\n",
      "[CV 2/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.650 total time=  11.4s\n",
      "[CV 3/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.635 total time=  13.2s\n",
      "[CV 4/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.645 total time=  12.7s\n",
      "[CV 5/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.642 total time=  12.9s\n",
      "[CV 1/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.639 total time=  11.8s\n",
      "[CV 2/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.650 total time=  13.7s\n",
      "[CV 3/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.635 total time=  15.6s\n",
      "[CV 4/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.645 total time=  12.5s\n",
      "[CV 5/5] END activation=tanh, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.642 total time=  15.3s\n",
      "[CV 1/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.640 total time=  16.1s\n",
      "[CV 2/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.642 total time=  16.3s\n",
      "[CV 3/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.633 total time=  11.0s\n",
      "[CV 4/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.632 total time=  14.0s\n",
      "[CV 5/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.632 total time=  14.2s\n",
      "[CV 1/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.640 total time=  15.3s\n",
      "[CV 2/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.642 total time=  17.4s\n",
      "[CV 3/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.633 total time=   9.2s\n",
      "[CV 4/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.632 total time=  12.2s\n",
      "[CV 5/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.632 total time=  12.8s\n",
      "[CV 1/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.640 total time=  11.9s\n",
      "[CV 2/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.648 total time=  12.2s\n",
      "[CV 3/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.630 total time=  12.9s\n",
      "[CV 4/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.639 total time=  13.1s\n",
      "[CV 5/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.637 total time=  11.8s\n",
      "[CV 1/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.640 total time=  12.8s\n",
      "[CV 2/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.648 total time=  12.6s\n",
      "[CV 3/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.630 total time=  12.2s\n",
      "[CV 4/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.639 total time=  13.6s\n",
      "[CV 5/5] END activation=tanh, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.637 total time=  12.4s\n",
      "[CV 1/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.638 total time=  19.0s\n",
      "[CV 2/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.637 total time=  18.5s\n",
      "[CV 3/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.632 total time=  20.7s\n",
      "[CV 4/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.626 total time=  18.2s\n",
      "[CV 5/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.635 total time=  19.4s\n",
      "[CV 1/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.638 total time=  19.5s\n",
      "[CV 2/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.637 total time=  19.4s\n",
      "[CV 3/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.632 total time=  19.5s\n",
      "[CV 4/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.626 total time=  17.7s\n",
      "[CV 5/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.635 total time=  17.2s\n",
      "[CV 1/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.635 total time=  23.2s\n",
      "[CV 2/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.644 total time=  25.5s\n",
      "[CV 3/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.637 total time=  20.6s\n",
      "[CV 4/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.632 total time=  19.3s\n",
      "[CV 5/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.638 total time=  16.1s\n",
      "[CV 1/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.635 total time=  23.0s\n",
      "[CV 2/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.644 total time=  25.0s\n",
      "[CV 3/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.637 total time=  19.4s\n",
      "[CV 4/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.632 total time=  18.2s\n",
      "[CV 5/5] END activation=tanh, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.638 total time=  17.3s\n",
      "[CV 1/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.643 total time=  22.3s\n",
      "[CV 2/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.641 total time=  18.8s\n",
      "[CV 3/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.633 total time=  21.5s\n",
      "[CV 4/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.635 total time=  15.9s\n",
      "[CV 5/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.633 total time=  15.9s\n",
      "[CV 1/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.643 total time=  22.7s\n",
      "[CV 2/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.641 total time=  18.6s\n",
      "[CV 3/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.633 total time=  18.9s\n",
      "[CV 4/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.635 total time=  16.2s\n",
      "[CV 5/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.633 total time=  14.8s\n",
      "[CV 1/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.640 total time=  19.0s\n",
      "[CV 2/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.634 total time=  29.9s\n",
      "[CV 3/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.639 total time=  40.9s\n",
      "[CV 4/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.639 total time=  34.8s\n",
      "[CV 5/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.638 total time=  31.6s\n",
      "[CV 1/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.640 total time=  25.1s\n",
      "[CV 2/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.634 total time=  42.2s\n",
      "[CV 3/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.639 total time=  34.9s\n",
      "[CV 4/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.639 total time=  28.9s\n",
      "[CV 5/5] END activation=logistic, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.638 total time=  26.6s\n",
      "[CV 1/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.637 total time=  20.1s\n",
      "[CV 2/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.644 total time=  20.9s\n",
      "[CV 3/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.634 total time=  19.6s\n",
      "[CV 4/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.638 total time=  23.4s\n",
      "[CV 5/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.635 total time=  19.1s\n",
      "[CV 1/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.637 total time=  18.8s\n",
      "[CV 2/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.644 total time=  19.9s\n",
      "[CV 3/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.634 total time=  22.6s\n",
      "[CV 4/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.638 total time=  25.4s\n",
      "[CV 5/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.635 total time=  18.6s\n",
      "[CV 1/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.638 total time=  24.5s\n",
      "[CV 2/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.643 total time=  24.0s\n",
      "[CV 3/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.634 total time=  27.9s\n",
      "[CV 4/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.636 total time=  36.5s\n",
      "[CV 5/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.635 total time=  39.6s\n",
      "[CV 1/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.638 total time=  30.1s\n",
      "[CV 2/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.643 total time=  26.0s\n",
      "[CV 3/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.634 total time=  27.1s\n",
      "[CV 4/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.636 total time=  31.5s\n",
      "[CV 5/5] END activation=logistic, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.635 total time=  34.4s\n",
      "[CV 1/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.632 total time=  20.3s\n",
      "[CV 2/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.642 total time=  23.7s\n",
      "[CV 3/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.630 total time=  20.2s\n",
      "[CV 4/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.634 total time=  34.1s\n",
      "[CV 5/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.632 total time=  27.3s\n",
      "[CV 1/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.632 total time=  20.9s\n",
      "[CV 2/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.642 total time=  24.2s\n",
      "[CV 3/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.630 total time=  20.3s\n",
      "[CV 4/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.634 total time=  34.0s\n",
      "[CV 5/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.632 total time=  27.2s\n",
      "[CV 1/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.638 total time=  34.1s\n",
      "[CV 2/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.646 total time=  41.2s\n",
      "[CV 3/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.633 total time=  34.8s\n",
      "[CV 4/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.639 total time=  48.7s\n",
      "[CV 5/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.639 total time=  49.8s\n",
      "[CV 1/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.638 total time=  34.5s\n",
      "[CV 2/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.646 total time=  40.9s\n",
      "[CV 3/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.633 total time=  34.2s\n",
      "[CV 4/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.639 total time=  48.7s\n",
      "[CV 5/5] END activation=logistic, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.639 total time=  49.4s\n",
      "[CV 1/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.622 total time=  13.0s\n",
      "[CV 2/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.627 total time=   9.4s\n",
      "[CV 3/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.623 total time=   7.0s\n",
      "[CV 4/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.626 total time=   9.0s\n",
      "[CV 5/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=True;, score=0.619 total time=  13.9s\n",
      "[CV 1/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.622 total time=  13.3s\n",
      "[CV 2/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.627 total time=   9.2s\n",
      "[CV 3/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.623 total time=   7.0s\n",
      "[CV 4/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.626 total time=   9.2s\n",
      "[CV 5/5] END activation=identity, batch_size=256, hidden_layer_sizes=(100,), warm_start=False;, score=0.619 total time=  14.1s\n",
      "[CV 1/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.627 total time=  16.7s\n",
      "[CV 2/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.628 total time=   7.1s\n",
      "[CV 3/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.620 total time=  20.3s\n",
      "[CV 4/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.623 total time=  20.3s\n",
      "[CV 5/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.631 total time=  23.7s\n",
      "[CV 1/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.627 total time=  16.2s\n",
      "[CV 2/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.628 total time=   7.4s\n",
      "[CV 3/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.620 total time=  20.5s\n",
      "[CV 4/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.623 total time=  20.3s\n",
      "[CV 5/5] END activation=identity, batch_size=256, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.631 total time=  23.9s\n",
      "[CV 1/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.626 total time=   5.8s\n",
      "[CV 2/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.627 total time=   6.5s\n",
      "[CV 3/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.626 total time=  10.7s\n",
      "[CV 4/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.622 total time=   5.8s\n",
      "[CV 5/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=True;, score=0.622 total time=   6.2s\n",
      "[CV 1/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.626 total time=   5.8s\n",
      "[CV 2/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.627 total time=   6.6s\n",
      "[CV 3/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.626 total time=  10.8s\n",
      "[CV 4/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.622 total time=   5.8s\n",
      "[CV 5/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(100,), warm_start=False;, score=0.622 total time=   6.4s\n",
      "[CV 1/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.621 total time=   9.3s\n",
      "[CV 2/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.632 total time=   8.9s\n",
      "[CV 3/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.620 total time=   9.1s\n",
      "[CV 4/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.628 total time=  11.3s\n",
      "[CV 5/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.625 total time=   5.6s\n",
      "[CV 1/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.621 total time=   9.0s\n",
      "[CV 2/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.632 total time=   8.9s\n",
      "[CV 3/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.620 total time=   9.0s\n",
      "[CV 4/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.628 total time=  11.1s\n",
      "[CV 5/5] END activation=identity, batch_size=1024, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.625 total time=   5.4s\n",
      "[CV 1/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.626 total time=   4.6s\n",
      "[CV 2/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.623 total time=   7.9s\n",
      "[CV 3/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.626 total time=  12.6s\n",
      "[CV 4/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.626 total time=  13.9s\n",
      "[CV 5/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=True;, score=0.629 total time=  13.5s\n",
      "[CV 1/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.626 total time=   4.6s\n",
      "[CV 2/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.623 total time=   7.9s\n",
      "[CV 3/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.626 total time=  12.5s\n",
      "[CV 4/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.626 total time=  13.2s\n",
      "[CV 5/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(100,), warm_start=False;, score=0.629 total time=  13.2s\n",
      "[CV 1/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.629 total time=   9.9s\n",
      "[CV 2/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.627 total time=  10.2s\n",
      "[CV 3/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.620 total time=  13.4s\n",
      "[CV 4/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.632 total time=   9.4s\n",
      "[CV 5/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=True;, score=0.631 total time=   8.2s\n",
      "[CV 1/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.629 total time=  10.2s\n",
      "[CV 2/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.627 total time=  10.4s\n",
      "[CV 3/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.620 total time=  13.4s\n",
      "[CV 4/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.632 total time=   9.4s\n",
      "[CV 5/5] END activation=identity, batch_size=4096, hidden_layer_sizes=(128, 64), warm_start=False;, score=0.631 total time=   8.2s\n",
      "{'activation': 'tanh', 'batch_size': 256, 'hidden_layer_sizes': (128, 64), 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'activation': ['relu', 'tanh','logistic','identity'],\n",
    "#     'warm_start': [True,False],\n",
    "#     'hidden_layer_sizes': [(100,),(128,64)],\n",
    "#     'batch_size':[256,1024,4096]\n",
    "# }\n",
    "\n",
    "# clf = GridSearchCV(estimator=MLPClassifier(early_stopping=True,random_state=42), param_grid=param_grid, scoring='f1_weighted',verbose=3)\n",
    "\n",
    "# clf.fit(X, y)\n",
    "# best_params = clf.best_params_\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified k-fold using best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.52      0.12      0.20       683\n",
      "        misc       0.75      0.43      0.55       960\n",
      "         pop       0.61      0.83      0.70     11127\n",
      "         rap       0.82      0.89      0.86      7710\n",
      "          rb       0.49      0.09      0.16      1236\n",
      "        rock       0.56      0.24      0.34      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.62      0.44      0.47     26781\n",
      "weighted avg       0.66      0.67      0.63     26781\n",
      "\n",
      "\n",
      "\n",
      "Fold 2:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.55      0.09      0.15       682\n",
      "        misc       0.75      0.40      0.52       960\n",
      "         pop       0.61      0.84      0.71     11127\n",
      "         rap       0.83      0.88      0.85      7710\n",
      "          rb       0.45      0.11      0.17      1237\n",
      "        rock       0.54      0.24      0.33      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.62      0.43      0.46     26781\n",
      "weighted avg       0.65      0.67      0.63     26781\n",
      "\n",
      "\n",
      "\n",
      "Fold 3:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.51      0.16      0.24       682\n",
      "        misc       0.72      0.45      0.56       960\n",
      "         pop       0.62      0.81      0.70     11127\n",
      "         rap       0.82      0.89      0.86      7710\n",
      "          rb       0.44      0.11      0.18      1237\n",
      "        rock       0.53      0.29      0.37      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.61      0.45      0.48     26781\n",
      "weighted avg       0.65      0.67      0.64     26781\n",
      "\n",
      "\n",
      "\n",
      "Fold 4:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.46      0.15      0.22       682\n",
      "        misc       0.72      0.43      0.54       960\n",
      "         pop       0.61      0.82      0.70     11127\n",
      "         rap       0.81      0.89      0.85      7710\n",
      "          rb       0.39      0.06      0.10      1237\n",
      "        rock       0.54      0.28      0.37      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.59      0.44      0.46     26781\n",
      "weighted avg       0.65      0.67      0.64     26781\n",
      "\n",
      "\n",
      "\n",
      "Fold 5:\n",
      "Training data: 107124 107124\n",
      "Testing data: 26781 26781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.51      0.09      0.15       682\n",
      "        misc       0.70      0.44      0.54       961\n",
      "         pop       0.60      0.85      0.71     11127\n",
      "         rap       0.85      0.86      0.86      7710\n",
      "          rb       0.39      0.06      0.10      1236\n",
      "        rock       0.55      0.24      0.34      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.60      0.42      0.45     26781\n",
      "weighted avg       0.65      0.67      0.63     26781\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the folds\n",
    "for fold, (train_idx, val_idx) in enumerate(stratified_kfold.split(X, y)):\n",
    "    # Extract the training and testing data for this fold\n",
    "    try:\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "    except:\n",
    "        X = np.asarray(X)\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Now you can train and evaluate your model on this fold\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(\"Training data:\", X_train.shape[0], len(y_train))\n",
    "    print(\"Testing data:\", X_val.shape[0], len(y_val))\n",
    "    # Your model training and evaluation code goes here\n",
    "    best_params = {'activation': 'tanh', 'batch_size': 256, 'hidden_layer_sizes': (128, 64), 'warm_start': True}\n",
    "    mlp = MLPClassifier(**best_params,early_stopping=True,random_state=42)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    predictions = mlp.predict(X_val)\n",
    "    print(classification_report(y_val, predictions))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model is the one that scored the highest on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.971838Z",
     "start_time": "2023-12-21T12:15:11.971818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.44      0.15      0.22       682\n",
      "        misc       0.70      0.44      0.54       961\n",
      "         pop       0.62      0.82      0.70     11127\n",
      "         rap       0.83      0.89      0.86      7710\n",
      "          rb       0.38      0.09      0.14      1236\n",
      "        rock       0.55      0.29      0.38      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.59      0.44      0.47     26781\n",
      "weighted avg       0.65      0.67      0.64     26781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_params = {'activation': 'logistic', 'batch_size': 256, 'hidden_layer_sizes': (128, 64), 'warm_start': True}\n",
    "nn = MLPClassifier(**best_params,random_state=42,early_stopping=True) \n",
    "nn.fit(X_train,y_train)\n",
    "val_predictions = nn.predict(X_val)\n",
    "print(classification_report(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.44      0.15      0.22       682\n",
      "        misc       0.70      0.44      0.54       961\n",
      "         pop       0.62      0.82      0.70     11127\n",
      "         rap       0.83      0.89      0.86      7710\n",
      "          rb       0.38      0.09      0.14      1236\n",
      "        rock       0.55      0.29      0.38      5065\n",
      "\n",
      "    accuracy                           0.67     26781\n",
      "   macro avg       0.59      0.44      0.47     26781\n",
      "weighted avg       0.65      0.67      0.64     26781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_params = {'activation': 'logistic', 'batch_size': 256, 'hidden_layer_sizes': (128, 64), 'warm_start': True}\n",
    "nn = MLPClassifier(**best_params,random_state=42,early_stopping=True) \n",
    "nn.fit(X_train,y_train)\n",
    "val_predictions = nn.predict(X_val)\n",
    "print(classification_report(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to train predictions to make sure it's not overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.55      0.19      0.28      2729\n",
      "        misc       0.78      0.49      0.60      3840\n",
      "         pop       0.62      0.83      0.71     44508\n",
      "         rap       0.85      0.90      0.87     30840\n",
      "          rb       0.50      0.11      0.18      4947\n",
      "        rock       0.57      0.30      0.40     20260\n",
      "\n",
      "    accuracy                           0.69    107124\n",
      "   macro avg       0.65      0.47      0.51    107124\n",
      "weighted avg       0.68      0.69      0.66    107124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_train = nn.predict(X_train)\n",
    "print(classification_report(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#FF914D'><font>**Neural networks from keras** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(units=64, activation='relu'))  # Hidden layer with 4 units and ReLU activation\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(units=24, activation='relu'))  # Hidden layer with 4 units and ReLU activation\n",
    "\n",
    "# model2.add(Dense(units=16, activation='relu'))  # Hidden layer with 4 units and ReLU activation\n",
    "\n",
    "model2.add(Dense(units=6, activation='softmax'))  # Output layer with 1 unit and sigmoid activation for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding data as keras neural networks don't work with strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = LabelEncoder()\n",
    "# y_nn = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X, y_nn, test_size=0.2, random_state=42, stratify = y_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.9608 - accuracy: 0.6412 - val_loss: 0.9040 - val_accuracy: 0.6539\n",
      "Epoch 2/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8890 - accuracy: 0.6602 - val_loss: 0.8925 - val_accuracy: 0.6552\n",
      "Epoch 3/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8730 - accuracy: 0.6649 - val_loss: 0.8846 - val_accuracy: 0.6593\n",
      "Epoch 4/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8596 - accuracy: 0.6676 - val_loss: 0.8841 - val_accuracy: 0.6598\n",
      "Epoch 5/250\n",
      "2679/2679 [==============================] - 7s 2ms/step - loss: 0.8514 - accuracy: 0.6705 - val_loss: 0.8820 - val_accuracy: 0.6604\n",
      "Epoch 6/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8407 - accuracy: 0.6736 - val_loss: 0.8808 - val_accuracy: 0.6590\n",
      "Epoch 7/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8364 - accuracy: 0.6744 - val_loss: 0.8847 - val_accuracy: 0.6590\n",
      "Epoch 8/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8284 - accuracy: 0.6759 - val_loss: 0.8874 - val_accuracy: 0.6580\n",
      "Epoch 9/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8232 - accuracy: 0.6790 - val_loss: 0.8873 - val_accuracy: 0.6598\n",
      "Epoch 10/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8200 - accuracy: 0.6778 - val_loss: 0.8872 - val_accuracy: 0.6592\n",
      "Epoch 11/250\n",
      "2679/2679 [==============================] - 7s 3ms/step - loss: 0.8156 - accuracy: 0.6794 - val_loss: 0.8902 - val_accuracy: 0.6568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16bacc113c0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model2.fit(X_train_nn, y_train_nn, epochs = 250,batch_size = 256, validation_split = 0.2,callbacks=[early_stopping],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837/837 [==============================] - 1s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.52      0.13      0.20       682\n",
      "        misc       0.69      0.37      0.48       960\n",
      "         pop       0.60      0.84      0.70     11127\n",
      "         rap       0.80      0.89      0.84      7710\n",
      "          rb       0.37      0.05      0.08      1237\n",
      "        rock       0.56      0.19      0.29      5065\n",
      "\n",
      "    accuracy                           0.66     26781\n",
      "   macro avg       0.59      0.41      0.43     26781\n",
      "weighted avg       0.64      0.66      0.61     26781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions_nn = model2.predict(X_test_nn)\n",
    "# print(classification_report(y_test_nn, predictions_nn.argmax(axis=1), target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3348/3348 [==============================] - 4s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.58      0.12      0.21      2729\n",
      "        misc       0.76      0.41      0.53      3841\n",
      "         pop       0.61      0.85      0.71     44508\n",
      "         rap       0.84      0.88      0.86     30840\n",
      "          rb       0.51      0.11      0.18      4946\n",
      "        rock       0.58      0.27      0.37     20260\n",
      "\n",
      "    accuracy                           0.68    107124\n",
      "   macro avg       0.65      0.44      0.48    107124\n",
      "weighted avg       0.67      0.68      0.65    107124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predictions_train_nn = model2.predict(X_train_nn)\n",
    "# print(classification_report(y_train_nn, predictions_train_nn.argmax(axis=1), target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class='anchor' id='13'></a>\n",
    "<font color='#FE7845'><font><h2>6. Genre Identification</h2></font>\n",
    "[Top &#129033;](#0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.981905Z",
     "start_time": "2023-12-21T12:15:11.981883Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the same preprocessing of the train data to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['lemmatized_lyrics'] = test_data['lyrics'].apply(lambda text: preprocessor(text,\n",
    "                 lowercase = True, \n",
    "                 leave_punctuation = False, \n",
    "                 remove_stopwords = True,\n",
    "                 stop_words=stop_words,\n",
    "                 correct_spelling = False, \n",
    "                 lemmatization = True, \n",
    "                 porter_stemming = False,\n",
    "                 tokenized_output = True, \n",
    "                 sentence_output = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.983934Z",
     "start_time": "2023-12-21T12:15:11.983917Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_data['stemmed_lyrics'] = test_data['lyrics'].apply(lambda text: preprocessor(text,\n",
    "#                  lowercase = True, \n",
    "#                  leave_punctuation = False, \n",
    "#                  remove_stopwords = True,\n",
    "#                  stop_words=stop_words,\n",
    "#                  correct_spelling = False, \n",
    "#                  lemmatization = False, \n",
    "#                  porter_stemming = True,\n",
    "#                  tokenized_output = True, \n",
    "#                  sentence_output = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data['clean_lyrics'] = test_data['lyrics'].apply(lambda text: preprocessor(text,\n",
    "#                  lowercase = True, \n",
    "#                  leave_punctuation = False, \n",
    "#                  remove_stopwords = True,\n",
    "#                  stop_words=stop_words,\n",
    "#                  correct_spelling = False, \n",
    "#                  lemmatization = False, \n",
    "#                  porter_stemming = False,\n",
    "#                  tokenized_output = True, \n",
    "#                  sentence_output = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing to export the submissions when using BoW or TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.985724Z",
     "start_time": "2023-12-21T12:15:11.985706Z"
    }
   },
   "outputs": [],
   "source": [
    "# corpus = [' '.join(tokens) for tokens in test_data['lemmatized_lyrics']]\n",
    "\n",
    "# X_test = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:29.813069Z",
     "start_time": "2023-12-21T12:15:29.808311Z"
    }
   },
   "source": [
    "Preparing to export the submissions when using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.987922Z",
     "start_time": "2023-12-21T12:15:11.987903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 33742/33742 [05:16<00:00, 106.53document/s]\n"
     ]
    }
   ],
   "source": [
    "test_corpus = list(read_corpus(test_data['lemmatized_lyrics'], tokens_only=True))\n",
    "X_test = []\n",
    "\n",
    "for i in tqdm(range(len(test_corpus)), desc=\"Processing\", unit=\"document\"):\n",
    "    X_test.append(model.infer_vector(test_corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class='anchor' id='14'></a>\n",
    "<font color='#FE7845'><font><h2>7. Export Results</h2></font>\n",
    "[Top &#129033;](#0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-21T12:15:11.992057Z",
     "start_time": "2023-12-21T12:15:11.992035Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = nn.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'tag': predictions})\n",
    "submission.to_csv('data/Group02__Version29.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing predictions when neural network from keras was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055/1055 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# X_test = np.asarray(X_test)\n",
    "# predictions = model2.predict(X_test)\n",
    "# predictions = predictions.argmax(axis=1)\n",
    "# predictions = le.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# submission = pd.DataFrame({'id': test_data['id'], 'tag': predictions})\n",
    "# submission.to_csv('data/Group02__Version25.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
